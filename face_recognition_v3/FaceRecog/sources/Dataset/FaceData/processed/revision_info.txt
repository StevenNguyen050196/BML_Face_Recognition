arguments: align_dataset_mtcnn.py Dataset\FaceData\raw Dataset\FaceData\processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.4.0
--------------------
git hash: b'eb5b26bc7983f51a90fd43e9413dc038d4cd952a'
--------------------
b'diff --git a/requirements.txt b/requirements.txt\nindex d16ee22..64b6459 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,5 +1,5 @@\n tensorflow>=1.12.1\n-scipy==1.1.0\n+scipy\n scikit-learn\n opencv-python\n h5py\ndiff --git a/src/__init__.py b/src/__init__.py\ndeleted file mode 100644\nindex efa6252..0000000\n--- a/src/__init__.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-# flake8: noqa\n-\ndiff --git a/src/a b/src/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/src/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/src/align/a b/src/align/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/src/align/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/src/align/det1.npy b/src/align/det1.npy\ndeleted file mode 100644\nindex 7c05a2c..0000000\nBinary files a/src/align/det1.npy and /dev/null differ\ndiff --git a/src/align/det2.npy b/src/align/det2.npy\ndeleted file mode 100644\nindex 85d5bf0..0000000\nBinary files a/src/align/det2.npy and /dev/null differ\ndiff --git a/src/align/det3.npy b/src/align/det3.npy\ndeleted file mode 100644\nindex 90d5ba9..0000000\nBinary files a/src/align/det3.npy and /dev/null differ\ndiff --git a/src/align/detect_face.py b/src/align/detect_face.py\ndeleted file mode 100644\nindex 7f98ca7..0000000\n--- a/src/align/detect_face.py\n+++ /dev/null\n@@ -1,781 +0,0 @@\n-""" Tensorflow implementation of the face detection / alignment algorithm found at\n-https://github.com/kpzhang93/MTCNN_face_detection_alignment\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-from six import string_types, iteritems\n-\n-import numpy as np\n-import tensorflow as tf\n-#from math import floor\n-import cv2\n-import os\n-\n-def layer(op):\n-    """Decorator for composable network layers."""\n-\n-    def layer_decorated(self, *args, **kwargs):\n-        # Automatically set a name if not provided.\n-        name = kwargs.setdefault(\'name\', self.get_unique_name(op.__name__))\n-        # Figure out the layer inputs.\n-        if len(self.terminals) == 0:\n-            raise RuntimeError(\'No input variables found for layer %s.\' % name)\n-        elif len(self.terminals) == 1:\n-            layer_input = self.terminals[0]\n-        else:\n-            layer_input = list(self.terminals)\n-        # Perform the operation and get the output.\n-        layer_output = op(self, layer_input, *args, **kwargs)\n-        # Add to layer LUT.\n-        self.layers[name] = layer_output\n-        # This output is now the input for the next layer.\n-        self.feed(layer_output)\n-        # Return self for chained calls.\n-        return self\n-\n-    return layer_decorated\n-\n-class Network(object):\n-\n-    def __init__(self, inputs, trainable=True):\n-        # The input nodes for this network\n-        self.inputs = inputs\n-        # The current list of terminal nodes\n-        self.terminals = []\n-        # Mapping from layer names to layers\n-        self.layers = dict(inputs)\n-        # If true, the resulting variables are set as trainable\n-        self.trainable = trainable\n-\n-        self.setup()\n-\n-    def setup(self):\n-        """Construct the network. """\n-        raise NotImplementedError(\'Must be implemented by the subclass.\')\n-\n-    def load(self, data_path, session, ignore_missing=False):\n-        """Load network weights.\n-        data_path: The path to the numpy-serialized network weights\n-        session: The current TensorFlow session\n-        ignore_missing: If true, serialized weights for missing layers are ignored.\n-        """\n-        data_dict = np.load(data_path, encoding=\'latin1\').item() #pylint: disable=no-member\n-\n-        for op_name in data_dict:\n-            with tf.variable_scope(op_name, reuse=True):\n-                for param_name, data in iteritems(data_dict[op_name]):\n-                    try:\n-                        var = tf.get_variable(param_name)\n-                        session.run(var.assign(data))\n-                    except ValueError:\n-                        if not ignore_missing:\n-                            raise\n-\n-    def feed(self, *args):\n-        """Set the input(s) for the next operation by replacing the terminal nodes.\n-        The arguments can be either layer names or the actual layers.\n-        """\n-        assert len(args) != 0\n-        self.terminals = []\n-        for fed_layer in args:\n-            if isinstance(fed_layer, string_types):\n-                try:\n-                    fed_layer = self.layers[fed_layer]\n-                except KeyError:\n-                    raise KeyError(\'Unknown layer name fed: %s\' % fed_layer)\n-            self.terminals.append(fed_layer)\n-        return self\n-\n-    def get_output(self):\n-        """Returns the current network output."""\n-        return self.terminals[-1]\n-\n-    def get_unique_name(self, prefix):\n-        """Returns an index-suffixed unique name for the given prefix.\n-        This is used for auto-generating layer names based on the type-prefix.\n-        """\n-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1\n-        return \'%s_%d\' % (prefix, ident)\n-\n-    def make_var(self, name, shape):\n-        """Creates a new TensorFlow variable."""\n-        return tf.get_variable(name, shape, trainable=self.trainable)\n-\n-    def validate_padding(self, padding):\n-        """Verifies that the padding is one of the supported ones."""\n-        assert padding in (\'SAME\', \'VALID\')\n-\n-    @layer\n-    def conv(self,\n-             inp,\n-             k_h,\n-             k_w,\n-             c_o,\n-             s_h,\n-             s_w,\n-             name,\n-             relu=True,\n-             padding=\'SAME\',\n-             group=1,\n-             biased=True):\n-        # Verify that the padding is acceptable\n-        self.validate_padding(padding)\n-        # Get the number of channels in the input\n-        c_i = int(inp.get_shape()[-1])\n-        # Verify that the grouping parameter is valid\n-        assert c_i % group == 0\n-        assert c_o % group == 0\n-        # Convolution for a given input and kernel\n-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n-        with tf.variable_scope(name) as scope:\n-            kernel = self.make_var(\'weights\', shape=[k_h, k_w, c_i // group, c_o])\n-            # This is the common-case. Convolve the input without any further complications.\n-            output = convolve(inp, kernel)\n-            # Add the biases\n-            if biased:\n-                biases = self.make_var(\'biases\', [c_o])\n-                output = tf.nn.bias_add(output, biases)\n-            if relu:\n-                # ReLU non-linearity\n-                output = tf.nn.relu(output, name=scope.name)\n-            return output\n-\n-    @layer\n-    def prelu(self, inp, name):\n-        with tf.variable_scope(name):\n-            i = int(inp.get_shape()[-1])\n-            alpha = self.make_var(\'alpha\', shape=(i,))\n-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))\n-        return output\n-\n-    @layer\n-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding=\'SAME\'):\n-        self.validate_padding(padding)\n-        return tf.nn.max_pool(inp,\n-                              ksize=[1, k_h, k_w, 1],\n-                              strides=[1, s_h, s_w, 1],\n-                              padding=padding,\n-                              name=name)\n-\n-    @layer\n-    def fc(self, inp, num_out, name, relu=True):\n-        with tf.variable_scope(name):\n-            input_shape = inp.get_shape()\n-            if input_shape.ndims == 4:\n-                # The input is spatial. Vectorize it first.\n-                dim = 1\n-                for d in input_shape[1:].as_list():\n-                    dim *= int(d)\n-                feed_in = tf.reshape(inp, [-1, dim])\n-            else:\n-                feed_in, dim = (inp, input_shape[-1].value)\n-            weights = self.make_var(\'weights\', shape=[dim, num_out])\n-            biases = self.make_var(\'biases\', [num_out])\n-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b\n-            fc = op(feed_in, weights, biases, name=name)\n-            return fc\n-\n-\n-    """\n-    Multi dimensional softmax,\n-    refer to https://github.com/tensorflow/tensorflow/issues/210\n-    compute softmax along the dimension of target\n-    the native softmax only supports batch_size x dimension\n-    """\n-    @layer\n-    def softmax(self, target, axis, name=None):\n-        max_axis = tf.reduce_max(target, axis, keepdims=True)\n-        target_exp = tf.exp(target-max_axis)\n-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)\n-        softmax = tf.div(target_exp, normalize, name)\n-        return softmax\n-    \n-class PNet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 10, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'PReLU1\')\n-             .max_pool(2, 2, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 16, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'PReLU2\')\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'PReLU3\')\n-             .conv(1, 1, 2, 1, 1, relu=False, name=\'conv4-1\')\n-             .softmax(3,name=\'prob1\'))\n-\n-        (self.feed(\'PReLU3\') #pylint: disable=no-value-for-parameter\n-             .conv(1, 1, 4, 1, 1, relu=False, name=\'conv4-2\'))\n-        \n-class RNet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 28, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 48, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(2, 2, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .fc(128, relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(2, relu=False, name=\'conv5-1\')\n-             .softmax(1,name=\'prob1\'))\n-\n-        (self.feed(\'prelu4\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv5-2\'))\n-\n-class ONet(Network):\n-    def setup(self):\n-        (self.feed(\'data\') #pylint: disable=no-value-for-parameter, no-member\n-             .conv(3, 3, 32, 1, 1, padding=\'VALID\', relu=False, name=\'conv1\')\n-             .prelu(name=\'prelu1\')\n-             .max_pool(3, 3, 2, 2, name=\'pool1\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv2\')\n-             .prelu(name=\'prelu2\')\n-             .max_pool(3, 3, 2, 2, padding=\'VALID\', name=\'pool2\')\n-             .conv(3, 3, 64, 1, 1, padding=\'VALID\', relu=False, name=\'conv3\')\n-             .prelu(name=\'prelu3\')\n-             .max_pool(2, 2, 2, 2, name=\'pool3\')\n-             .conv(2, 2, 128, 1, 1, padding=\'VALID\', relu=False, name=\'conv4\')\n-             .prelu(name=\'prelu4\')\n-             .fc(256, relu=False, name=\'conv5\')\n-             .prelu(name=\'prelu5\')\n-             .fc(2, relu=False, name=\'conv6-1\')\n-             .softmax(1, name=\'prob1\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(4, relu=False, name=\'conv6-2\'))\n-\n-        (self.feed(\'prelu5\') #pylint: disable=no-value-for-parameter\n-             .fc(10, relu=False, name=\'conv6-3\'))\n-\n-def create_mtcnn(sess, model_path):\n-    if not model_path:\n-        model_path,_ = os.path.split(os.path.realpath(__file__))\n-\n-    with tf.variable_scope(\'pnet\'):\n-        data = tf.placeholder(tf.float32, (None,None,None,3), \'input\')\n-        pnet = PNet({\'data\':data})\n-        pnet.load(os.path.join(model_path, \'det1.npy\'), sess)\n-    with tf.variable_scope(\'rnet\'):\n-        data = tf.placeholder(tf.float32, (None,24,24,3), \'input\')\n-        rnet = RNet({\'data\':data})\n-        rnet.load(os.path.join(model_path, \'det2.npy\'), sess)\n-    with tf.variable_scope(\'onet\'):\n-        data = tf.placeholder(tf.float32, (None,48,48,3), \'input\')\n-        onet = ONet({\'data\':data})\n-        onet.load(os.path.join(model_path, \'det3.npy\'), sess)\n-        \n-    pnet_fun = lambda img : sess.run((\'pnet/conv4-2/BiasAdd:0\', \'pnet/prob1:0\'), feed_dict={\'pnet/input:0\':img})\n-    rnet_fun = lambda img : sess.run((\'rnet/conv5-2/conv5-2:0\', \'rnet/prob1:0\'), feed_dict={\'rnet/input:0\':img})\n-    onet_fun = lambda img : sess.run((\'onet/conv6-2/conv6-2:0\', \'onet/conv6-3/conv6-3:0\', \'onet/prob1:0\'), feed_dict={\'onet/input:0\':img})\n-    return pnet_fun, rnet_fun, onet_fun\n-\n-def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):\n-    """Detects faces in an image, and returns bounding boxes and points for them.\n-    img: input image\n-    minsize: minimum faces\' size\n-    pnet, rnet, onet: caffemodel\n-    threshold: threshold=[th1, th2, th3], th1-3 are three steps\'s threshold\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n-    """\n-    factor_count=0\n-    total_boxes=np.empty((0,9))\n-    points=np.empty(0)\n-    h=img.shape[0]\n-    w=img.shape[1]\n-    minl=np.amin([h, w])\n-    m=12.0/minsize\n-    minl=minl*m\n-    # create scale pyramid\n-    scales=[]\n-    while minl>=12:\n-        scales += [m*np.power(factor, factor_count)]\n-        minl = minl*factor\n-        factor_count += 1\n-\n-    # first stage\n-    for scale in scales:\n-        hs=int(np.ceil(h*scale))\n-        ws=int(np.ceil(w*scale))\n-        im_data = imresample(img, (hs, ws))\n-        im_data = (im_data-127.5)*0.0078125\n-        img_x = np.expand_dims(im_data, 0)\n-        img_y = np.transpose(img_x, (0,2,1,3))\n-        out = pnet(img_y)\n-        out0 = np.transpose(out[0], (0,2,1,3))\n-        out1 = np.transpose(out[1], (0,2,1,3))\n-        \n-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])\n-        \n-        # inter-scale nms\n-        pick = nms(boxes.copy(), 0.5, \'Union\')\n-        if boxes.size>0 and pick.size>0:\n-            boxes = boxes[pick,:]\n-            total_boxes = np.append(total_boxes, boxes, axis=0)\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        pick = nms(total_boxes.copy(), 0.7, \'Union\')\n-        total_boxes = total_boxes[pick,:]\n-        regw = total_boxes[:,2]-total_boxes[:,0]\n-        regh = total_boxes[:,3]-total_boxes[:,1]\n-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw\n-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh\n-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw\n-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh\n-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))\n-        total_boxes = rerec(total_boxes.copy())\n-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        # second stage\n-        tempimg = np.zeros((24,24,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))\n-            else:\n-                return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n-        out = rnet(tempimg1)\n-        out0 = np.transpose(out[0])\n-        out1 = np.transpose(out[1])\n-        score = out1[1,:]\n-        ipass = np.where(score>threshold[1])\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-        if total_boxes.shape[0]>0:\n-            pick = nms(total_boxes, 0.7, \'Union\')\n-            total_boxes = total_boxes[pick,:]\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))\n-            total_boxes = rerec(total_boxes.copy())\n-\n-    numbox = total_boxes.shape[0]\n-    if numbox>0:\n-        # third stage\n-        total_boxes = np.fix(total_boxes).astype(np.int32)\n-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)\n-        tempimg = np.zeros((48,48,3,numbox))\n-        for k in range(0,numbox):\n-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))\n-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]\n-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:\n-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))\n-            else:\n-                return np.empty()\n-        tempimg = (tempimg-127.5)*0.0078125\n-        tempimg1 = np.transpose(tempimg, (3,1,0,2))\n-        out = onet(tempimg1)\n-        out0 = np.transpose(out[0])\n-        out1 = np.transpose(out[1])\n-        out2 = np.transpose(out[2])\n-        score = out2[1,:]\n-        points = out1\n-        ipass = np.where(score>threshold[2])\n-        points = points[:,ipass[0]]\n-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])\n-        mv = out0[:,ipass[0]]\n-\n-        w = total_boxes[:,2]-total_boxes[:,0]+1\n-        h = total_boxes[:,3]-total_boxes[:,1]+1\n-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1\n-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1\n-        if total_boxes.shape[0]>0:\n-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))\n-            pick = nms(total_boxes.copy(), 0.7, \'Min\')\n-            total_boxes = total_boxes[pick,:]\n-            points = points[:,pick]\n-                \n-    return total_boxes, points\n-\n-\n-def bulk_detect_face(images, detection_window_size_ratio, pnet, rnet, onet, threshold, factor):\n-    """Detects faces in a list of images\n-    images: list containing input images\n-    detection_window_size_ratio: ratio of minimum face size to smallest image dimension\n-    pnet, rnet, onet: caffemodel\n-    threshold: threshold=[th1 th2 th3], th1-3 are three steps\'s threshold [0-1]\n-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.\n-    """\n-    all_scales = [None] * len(images)\n-    images_with_boxes = [None] * len(images)\n-\n-    for i in range(len(images)):\n-        images_with_boxes[i] = {\'total_boxes\': np.empty((0, 9))}\n-\n-    # create scale pyramid\n-    for index, img in enumerate(images):\n-        all_scales[index] = []\n-        h = img.shape[0]\n-        w = img.shape[1]\n-        minsize = int(detection_window_size_ratio * np.minimum(w, h))\n-        factor_count = 0\n-        minl = np.amin([h, w])\n-        if minsize <= 12:\n-            minsize = 12\n-\n-        m = 12.0 / minsize\n-        minl = minl * m\n-        while minl >= 12:\n-            all_scales[index].append(m * np.power(factor, factor_count))\n-            minl = minl * factor\n-            factor_count += 1\n-\n-    # # # # # # # # # # # # #\n-    # first stage - fast proposal network (pnet) to obtain face candidates\n-    # # # # # # # # # # # # #\n-\n-    images_obj_per_resolution = {}\n-\n-    # TODO: use some type of rounding to number module 8 to increase probability that pyramid images will have the same resolution across input images\n-\n-    for index, scales in enumerate(all_scales):\n-        h = images[index].shape[0]\n-        w = images[index].shape[1]\n-\n-        for scale in scales:\n-            hs = int(np.ceil(h * scale))\n-            ws = int(np.ceil(w * scale))\n-\n-            if (ws, hs) not in images_obj_per_resolution:\n-                images_obj_per_resolution[(ws, hs)] = []\n-\n-            im_data = imresample(images[index], (hs, ws))\n-            im_data = (im_data - 127.5) * 0.0078125\n-            img_y = np.transpose(im_data, (1, 0, 2))  # caffe uses different dimensions ordering\n-            images_obj_per_resolution[(ws, hs)].append({\'scale\': scale, \'image\': img_y, \'index\': index})\n-\n-    for resolution in images_obj_per_resolution:\n-        images_per_resolution = [i[\'image\'] for i in images_obj_per_resolution[resolution]]\n-        outs = pnet(images_per_resolution)\n-\n-        for index in range(len(outs[0])):\n-            scale = images_obj_per_resolution[resolution][index][\'scale\']\n-            image_index = images_obj_per_resolution[resolution][index][\'index\']\n-            out0 = np.transpose(outs[0][index], (1, 0, 2))\n-            out1 = np.transpose(outs[1][index], (1, 0, 2))\n-\n-            boxes, _ = generateBoundingBox(out1[:, :, 1].copy(), out0[:, :, :].copy(), scale, threshold[0])\n-\n-            # inter-scale nms\n-            pick = nms(boxes.copy(), 0.5, \'Union\')\n-            if boxes.size > 0 and pick.size > 0:\n-                boxes = boxes[pick, :]\n-                images_with_boxes[image_index][\'total_boxes\'] = np.append(images_with_boxes[image_index][\'total_boxes\'],\n-                                                                          boxes,\n-                                                                          axis=0)\n-\n-    for index, image_obj in enumerate(images_with_boxes):\n-        numbox = image_obj[\'total_boxes\'].shape[0]\n-        if numbox > 0:\n-            h = images[index].shape[0]\n-            w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            regw = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0]\n-            regh = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1]\n-            qq1 = image_obj[\'total_boxes\'][:, 0] + image_obj[\'total_boxes\'][:, 5] * regw\n-            qq2 = image_obj[\'total_boxes\'][:, 1] + image_obj[\'total_boxes\'][:, 6] * regh\n-            qq3 = image_obj[\'total_boxes\'][:, 2] + image_obj[\'total_boxes\'][:, 7] * regw\n-            qq4 = image_obj[\'total_boxes\'][:, 3] + image_obj[\'total_boxes\'][:, 8] * regh\n-            image_obj[\'total_boxes\'] = np.transpose(np.vstack([qq1, qq2, qq3, qq4, image_obj[\'total_boxes\'][:, 4]]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n-            image_obj[\'total_boxes\'][:, 0:4] = np.fix(image_obj[\'total_boxes\'][:, 0:4]).astype(np.int32)\n-            dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n-\n-            numbox = image_obj[\'total_boxes\'].shape[0]\n-            tempimg = np.zeros((24, 24, 3, numbox))\n-\n-            if numbox > 0:\n-                for k in range(0, numbox):\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n-                        tempimg[:, :, :, k] = imresample(tmp, (24, 24))\n-                    else:\n-                        return np.empty()\n-\n-                tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'rnet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n-\n-    # # # # # # # # # # # # #\n-    # second stage - refinement of face candidates with rnet\n-    # # # # # # # # # # # # #\n-\n-    bulk_rnet_input = np.empty((0, 24, 24, 3))\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' in image_obj:\n-            bulk_rnet_input = np.append(bulk_rnet_input, image_obj[\'rnet_input\'], axis=0)\n-\n-    out = rnet(bulk_rnet_input)\n-    out0 = np.transpose(out[0])\n-    out1 = np.transpose(out[1])\n-    score = out1[1, :]\n-\n-    i = 0\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'rnet_input\' not in image_obj:\n-            continue\n-\n-        rnet_input_count = image_obj[\'rnet_input\'].shape[0]\n-        score_per_image = score[i:i + rnet_input_count]\n-        out0_per_image = out0[:, i:i + rnet_input_count]\n-\n-        ipass = np.where(score_per_image > threshold[1])\n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n-\n-        mv = out0_per_image[:, ipass[0]]\n-\n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n-            h = images[index].shape[0]\n-            w = images[index].shape[1]\n-            pick = nms(image_obj[\'total_boxes\'], 0.7, \'Union\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv[:, pick]))\n-            image_obj[\'total_boxes\'] = rerec(image_obj[\'total_boxes\'].copy())\n-\n-            numbox = image_obj[\'total_boxes\'].shape[0]\n-\n-            if numbox > 0:\n-                tempimg = np.zeros((48, 48, 3, numbox))\n-                image_obj[\'total_boxes\'] = np.fix(image_obj[\'total_boxes\']).astype(np.int32)\n-                dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(image_obj[\'total_boxes\'].copy(), w, h)\n-\n-                for k in range(0, numbox):\n-                    tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))\n-                    tmp[dy[k] - 1:edy[k], dx[k] - 1:edx[k], :] = images[index][y[k] - 1:ey[k], x[k] - 1:ex[k], :]\n-                    if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:\n-                        tempimg[:, :, :, k] = imresample(tmp, (48, 48))\n-                    else:\n-                        return np.empty()\n-                tempimg = (tempimg - 127.5) * 0.0078125\n-                image_obj[\'onet_input\'] = np.transpose(tempimg, (3, 1, 0, 2))\n-\n-        i += rnet_input_count\n-\n-    # # # # # # # # # # # # #\n-    # third stage - further refinement and facial landmarks positions with onet\n-    # # # # # # # # # # # # #\n-\n-    bulk_onet_input = np.empty((0, 48, 48, 3))\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' in image_obj:\n-            bulk_onet_input = np.append(bulk_onet_input, image_obj[\'onet_input\'], axis=0)\n-\n-    out = onet(bulk_onet_input)\n-\n-    out0 = np.transpose(out[0])\n-    out1 = np.transpose(out[1])\n-    out2 = np.transpose(out[2])\n-    score = out2[1, :]\n-    points = out1\n-\n-    i = 0\n-    ret = []\n-    for index, image_obj in enumerate(images_with_boxes):\n-        if \'onet_input\' not in image_obj:\n-            ret.append(None)\n-            continue\n-\n-        onet_input_count = image_obj[\'onet_input\'].shape[0]\n-\n-        out0_per_image = out0[:, i:i + onet_input_count]\n-        score_per_image = score[i:i + onet_input_count]\n-        points_per_image = points[:, i:i + onet_input_count]\n-\n-        ipass = np.where(score_per_image > threshold[2])\n-        points_per_image = points_per_image[:, ipass[0]]\n-\n-        image_obj[\'total_boxes\'] = np.hstack([image_obj[\'total_boxes\'][ipass[0], 0:4].copy(),\n-                                              np.expand_dims(score_per_image[ipass].copy(), 1)])\n-        mv = out0_per_image[:, ipass[0]]\n-\n-        w = image_obj[\'total_boxes\'][:, 2] - image_obj[\'total_boxes\'][:, 0] + 1\n-        h = image_obj[\'total_boxes\'][:, 3] - image_obj[\'total_boxes\'][:, 1] + 1\n-        points_per_image[0:5, :] = np.tile(w, (5, 1)) * points_per_image[0:5, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 0], (5, 1)) - 1\n-        points_per_image[5:10, :] = np.tile(h, (5, 1)) * points_per_image[5:10, :] + np.tile(\n-            image_obj[\'total_boxes\'][:, 1], (5, 1)) - 1\n-\n-        if image_obj[\'total_boxes\'].shape[0] > 0:\n-            image_obj[\'total_boxes\'] = bbreg(image_obj[\'total_boxes\'].copy(), np.transpose(mv))\n-            pick = nms(image_obj[\'total_boxes\'].copy(), 0.7, \'Min\')\n-            image_obj[\'total_boxes\'] = image_obj[\'total_boxes\'][pick, :]\n-            points_per_image = points_per_image[:, pick]\n-\n-            ret.append((image_obj[\'total_boxes\'], points_per_image))\n-        else:\n-            ret.append(None)\n-\n-        i += onet_input_count\n-\n-    return ret\n-\n-\n-# function [boundingbox] = bbreg(boundingbox,reg)\n-def bbreg(boundingbox,reg):\n-    """Calibrate bounding boxes"""\n-    if reg.shape[1]==1:\n-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))\n-\n-    w = boundingbox[:,2]-boundingbox[:,0]+1\n-    h = boundingbox[:,3]-boundingbox[:,1]+1\n-    b1 = boundingbox[:,0]+reg[:,0]*w\n-    b2 = boundingbox[:,1]+reg[:,1]*h\n-    b3 = boundingbox[:,2]+reg[:,2]*w\n-    b4 = boundingbox[:,3]+reg[:,3]*h\n-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))\n-    return boundingbox\n- \n-def generateBoundingBox(imap, reg, scale, t):\n-    """Use heatmap to generate bounding boxes"""\n-    stride=2\n-    cellsize=12\n-\n-    imap = np.transpose(imap)\n-    dx1 = np.transpose(reg[:,:,0])\n-    dy1 = np.transpose(reg[:,:,1])\n-    dx2 = np.transpose(reg[:,:,2])\n-    dy2 = np.transpose(reg[:,:,3])\n-    y, x = np.where(imap >= t)\n-    if y.shape[0]==1:\n-        dx1 = np.flipud(dx1)\n-        dy1 = np.flipud(dy1)\n-        dx2 = np.flipud(dx2)\n-        dy2 = np.flipud(dy2)\n-    score = imap[(y,x)]\n-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))\n-    if reg.size==0:\n-        reg = np.empty((0,3))\n-    bb = np.transpose(np.vstack([y,x]))\n-    q1 = np.fix((stride*bb+1)/scale)\n-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)\n-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])\n-    return boundingbox, reg\n- \n-# function pick = nms(boxes,threshold,type)\n-def nms(boxes, threshold, method):\n-    if boxes.size==0:\n-        return np.empty((0,3))\n-    x1 = boxes[:,0]\n-    y1 = boxes[:,1]\n-    x2 = boxes[:,2]\n-    y2 = boxes[:,3]\n-    s = boxes[:,4]\n-    area = (x2-x1+1) * (y2-y1+1)\n-    I = np.argsort(s)\n-    pick = np.zeros_like(s, dtype=np.int16)\n-    counter = 0\n-    while I.size>0:\n-        i = I[-1]\n-        pick[counter] = i\n-        counter += 1\n-        idx = I[0:-1]\n-        xx1 = np.maximum(x1[i], x1[idx])\n-        yy1 = np.maximum(y1[i], y1[idx])\n-        xx2 = np.minimum(x2[i], x2[idx])\n-        yy2 = np.minimum(y2[i], y2[idx])\n-        w = np.maximum(0.0, xx2-xx1+1)\n-        h = np.maximum(0.0, yy2-yy1+1)\n-        inter = w * h\n-        if method is \'Min\':\n-            o = inter / np.minimum(area[i], area[idx])\n-        else:\n-            o = inter / (area[i] + area[idx] - inter)\n-        I = I[np.where(o<=threshold)]\n-    pick = pick[0:counter]\n-    return pick\n-\n-# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)\n-def pad(total_boxes, w, h):\n-    """Compute the padding coordinates (pad the bounding boxes to square)"""\n-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)\n-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)\n-    numbox = total_boxes.shape[0]\n-\n-    dx = np.ones((numbox), dtype=np.int32)\n-    dy = np.ones((numbox), dtype=np.int32)\n-    edx = tmpw.copy().astype(np.int32)\n-    edy = tmph.copy().astype(np.int32)\n-\n-    x = total_boxes[:,0].copy().astype(np.int32)\n-    y = total_boxes[:,1].copy().astype(np.int32)\n-    ex = total_boxes[:,2].copy().astype(np.int32)\n-    ey = total_boxes[:,3].copy().astype(np.int32)\n-\n-    tmp = np.where(ex>w)\n-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)\n-    ex[tmp] = w\n-    \n-    tmp = np.where(ey>h)\n-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)\n-    ey[tmp] = h\n-\n-    tmp = np.where(x<1)\n-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)\n-    x[tmp] = 1\n-\n-    tmp = np.where(y<1)\n-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)\n-    y[tmp] = 1\n-    \n-    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph\n-\n-# function [bboxA] = rerec(bboxA)\n-def rerec(bboxA):\n-    """Convert bboxA to square."""\n-    h = bboxA[:,3]-bboxA[:,1]\n-    w = bboxA[:,2]-bboxA[:,0]\n-    l = np.maximum(w, h)\n-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5\n-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5\n-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))\n-    return bboxA\n-\n-def imresample(img, sz):\n-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable\n-    return im_data\n-\n-    # This method is kept for debugging purpose\n-#     h=img.shape[0]\n-#     w=img.shape[1]\n-#     hs, ws = sz\n-#     dx = float(w) / ws\n-#     dy = float(h) / hs\n-#     im_data = np.zeros((hs,ws,3))\n-#     for a1 in range(0,hs):\n-#         for a2 in range(0,ws):\n-#             for a3 in range(0,3):\n-#                 im_data[a1,a2,a3] = img[int(floor(a1*dy)),int(floor(a2*dx)),a3]\n-#     return im_data\n-\ndiff --git a/src/align_dataset_mtcnn.py b/src/align_dataset_mtcnn.py\ndeleted file mode 100644\nindex 7d5e735..0000000\n--- a/src/align_dataset_mtcnn.py\n+++ /dev/null\n@@ -1,159 +0,0 @@\n-"""Performs face alignment and stores face thumbnails in the output directory."""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import sys\n-import os\n-import argparse\n-import tensorflow as tf\n-import numpy as np\n-import facenet\n-import align.detect_face\n-import random\n-from time import sleep\n-\n-def main(args):\n-    sleep(random.random())\n-    output_dir = os.path.expanduser(args.output_dir)\n-    if not os.path.exists(output_dir):\n-        os.makedirs(output_dir)\n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    dataset = facenet.get_dataset(args.input_dir)\n-    \n-    print(\'Creating networks and loading parameters\')\n-    \n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-    \n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-\n-    # Add a random key to the filename to allow alignment using multiple processes\n-    random_key = np.random.randint(0, high=99999)\n-    bounding_boxes_filename = os.path.join(output_dir, \'bounding_boxes_%05d.txt\' % random_key)\n-    \n-    with open(bounding_boxes_filename, "w") as text_file:\n-        nrof_images_total = 0\n-        nrof_successfully_aligned = 0\n-        if args.random_order:\n-            random.shuffle(dataset)\n-        for cls in dataset:\n-            output_class_dir = os.path.join(output_dir, cls.name)\n-            if not os.path.exists(output_class_dir):\n-                os.makedirs(output_class_dir)\n-                if args.random_order:\n-                    random.shuffle(cls.image_paths)\n-            for image_path in cls.image_paths:\n-                nrof_images_total += 1\n-                filename = os.path.splitext(os.path.split(image_path)[1])[0]\n-                output_filename = os.path.join(output_class_dir, filename+\'.png\')\n-                print(image_path)\n-                if not os.path.exists(output_filename):\n-                    try:\n-                        img = misc.imread(image_path)\n-                    except (IOError, ValueError, IndexError) as e:\n-                        errorMessage = \'{}: {}\'.format(image_path, e)\n-                        print(errorMessage)\n-                    else:\n-                        if img.ndim<2:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            continue\n-                        if img.ndim == 2:\n-                            img = facenet.to_rgb(img)\n-                        img = img[:,:,0:3]\n-    \n-                        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-                        nrof_faces = bounding_boxes.shape[0]\n-                        if nrof_faces>0:\n-                            det = bounding_boxes[:,0:4]\n-                            det_arr = []\n-                            img_size = np.asarray(img.shape)[0:2]\n-                            if nrof_faces>1:\n-                                if args.detect_multiple_faces:\n-                                    for i in range(nrof_faces):\n-                                        det_arr.append(np.squeeze(det[i]))\n-                                else:\n-                                    bounding_box_size = (det[:,2]-det[:,0])*(det[:,3]-det[:,1])\n-                                    img_center = img_size / 2\n-                                    offsets = np.vstack([ (det[:,0]+det[:,2])/2-img_center[1], (det[:,1]+det[:,3])/2-img_center[0] ])\n-                                    offset_dist_squared = np.sum(np.power(offsets,2.0),0)\n-                                    index = np.argmax(bounding_box_size-offset_dist_squared*2.0) # some extra weight on the centering\n-                                    det_arr.append(det[index,:])\n-                            else:\n-                                det_arr.append(np.squeeze(det))\n-\n-                            for i, det in enumerate(det_arr):\n-                                det = np.squeeze(det)\n-                                bb = np.zeros(4, dtype=np.int32)\n-                                bb[0] = np.maximum(det[0]-args.margin/2, 0)\n-                                bb[1] = np.maximum(det[1]-args.margin/2, 0)\n-                                bb[2] = np.minimum(det[2]+args.margin/2, img_size[1])\n-                                bb[3] = np.minimum(det[3]+args.margin/2, img_size[0])\n-                                cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-                                scaled = misc.imresize(cropped, (args.image_size, args.image_size), interp=\'bilinear\')\n-                                nrof_successfully_aligned += 1\n-                                filename_base, file_extension = os.path.splitext(output_filename)\n-                                if args.detect_multiple_faces:\n-                                    output_filename_n = "{}_{}{}".format(filename_base, i, file_extension)\n-                                else:\n-                                    output_filename_n = "{}{}".format(filename_base, file_extension)\n-                                misc.imsave(output_filename_n, scaled)\n-                                text_file.write(\'%s %d %d %d %d\\n\' % (output_filename_n, bb[0], bb[1], bb[2], bb[3]))\n-                        else:\n-                            print(\'Unable to align "%s"\' % image_path)\n-                            text_file.write(\'%s\\n\' % (output_filename))\n-                            \n-    print(\'Total number of images: %d\' % nrof_images_total)\n-    print(\'Number of successfully aligned images: %d\' % nrof_successfully_aligned)\n-            \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'input_dir\', type=str, help=\'Directory with unaligned images.\')\n-    parser.add_argument(\'output_dir\', type=str, help=\'Directory with aligned face thumbnails.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=182)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--random_order\', \n-        help=\'Shuffles the order of images to enable alignment using multiple processes.\', action=\'store_true\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--detect_multiple_faces\', type=bool,\n-                        help=\'Detect and align multiple faces per image.\', default=False)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\ndeleted file mode 100644\nindex f60b9ae..0000000\n--- a/src/calculate_filtering_metrics.py\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import os\n-import sys\n-import time\n-import h5py\n-import math\n-from tensorflow.python.platform import gfile\n-from six import iteritems\n-\n-def main(args):\n-    dataset = facenet.get_dataset(args.dataset_dir)\n-  \n-    with tf.Graph().as_default():\n-      \n-        # Get a list of image paths and their labels\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\n-        nrof_images = len(image_list)\n-        image_indices = range(nrof_images)\n-\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\n-            image_indices, args.image_size, args.batch_size, None, \n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\n-        \n-        model_exp = os.path.expanduser(args.model_file)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            input_map={\'input\':image_batch, \'phase_train\':False}\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'net\')\n-        \n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\n-\n-        with tf.Session() as sess:\n-            tf.train.start_queue_runners(sess=sess)\n-                \n-            embedding_size = int(embeddings.get_shape()[1])\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\n-            nrof_classes = len(dataset)\n-            label_array = np.array(label_list)\n-            class_names = [cls.name for cls in dataset]\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\n-            class_variance = np.zeros((nrof_classes,))\n-            class_center = np.zeros((nrof_classes,embedding_size))\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\n-            emb_array = np.zeros((0,embedding_size))\n-            idx_array = np.zeros((0,), dtype=np.int32)\n-            lab_array = np.zeros((0,), dtype=np.int32)\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\n-            for i in range(nrof_batches):\n-                t = time.time()\n-                emb, idx = sess.run([embeddings, label_batch])\n-                emb_array = np.append(emb_array, emb, axis=0)\n-                idx_array = np.append(idx_array, idx, axis=0)\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\n-                for cls in set(lab_array):\n-                    cls_idx = np.where(lab_array==cls)[0]\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\n-                        # We have calculated all the embeddings for this class\n-                        i2 = np.argsort(idx_array[cls_idx])\n-                        emb_class = emb_array[cls_idx,:]\n-                        emb_sort = emb_class[i2,:]\n-                        center = np.mean(emb_sort, axis=0)\n-                        diffs = emb_sort - center\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\n-                        class_variance[cls] = np.mean(dists_sqr)\n-                        class_center[cls,:] = center\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\n-\n-                        \n-                print(\'Batch %d in %.3f seconds\' % (i, time.time()-t))\n-                \n-            print(\'Writing filtering data to %s\' % args.data_file_name)\n-            mdict = {\'class_names\':class_names, \'image_list\':image_list, \'label_list\':label_list, \'distance_to_center\':distance_to_center }\n-            with h5py.File(args.data_file_name, \'w\') as f:\n-                for key, value in iteritems(mdict):\n-                    f.create_dataset(key, data=value)\n-                        \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'dataset_dir\', type=str,\n-        help=\'Path to the directory containing aligned dataset.\')\n-    parser.add_argument(\'model_file\', type=str,\n-        help=\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\')\n-    parser.add_argument(\'data_file_name\', type=str,\n-        help=\'The name of the file to store filtering data in.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size.\', default=160)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/classifier.py b/src/classifier.py\ndeleted file mode 100644\nindex e7189bc..0000000\n--- a/src/classifier.py\n+++ /dev/null\n@@ -1,170 +0,0 @@\n-"""An example of how to use your own dataset to train a classifier that recognizes people.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-from sklearn.svm import SVC\n-\n-def main(args):\n-  \n-    with tf.Graph().as_default():\n-      \n-        with tf.Session() as sess:\n-            \n-            np.random.seed(seed=args.seed)\n-            \n-            if args.use_split_dataset:\n-                dataset_tmp = facenet.get_dataset(args.data_dir)\n-                train_set, test_set = split_dataset(dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n-                if (args.mode==\'TRAIN\'):\n-                    dataset = train_set\n-                elif (args.mode==\'CLASSIFY\'):\n-                    dataset = test_set\n-            else:\n-                dataset = facenet.get_dataset(args.data_dir)\n-\n-            # Check that there are at least one training image per class\n-            for cls in dataset:\n-                assert(len(cls.image_paths)>0, \'There must be at least one image for each class in the dataset\')\n-\n-                 \n-            paths, labels = facenet.get_image_paths_and_labels(dataset)\n-            \n-            print(\'Number of classes: %d\' % len(dataset))\n-            print(\'Number of images: %d\' % len(paths))\n-            \n-            # Load the model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(args.model)\n-            \n-            # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-            \n-            # Run forward pass to calculate embeddings\n-            print(\'Calculating features for images\')\n-            nrof_images = len(paths)\n-            nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / args.batch_size))\n-            emb_array = np.zeros((nrof_images, embedding_size))\n-            for i in range(nrof_batches_per_epoch):\n-                start_index = i*args.batch_size\n-                end_index = min((i+1)*args.batch_size, nrof_images)\n-                paths_batch = paths[start_index:end_index]\n-                images = facenet.load_data(paths_batch, False, False, args.image_size)\n-                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n-\n-            if (args.mode==\'TRAIN\'):\n-                # Train classifier\n-                print(\'Training classifier\')\n-                model = SVC(kernel=\'linear\', probability=True)\n-                model.fit(emb_array, labels)\n-            \n-                # Create a list of class names\n-                class_names = [ cls.name.replace(\'_\', \' \') for cls in dataset]\n-\n-                # Saving classifier model\n-                with open(classifier_filename_exp, \'wb\') as outfile:\n-                    pickle.dump((model, class_names), outfile)\n-                print(\'Saved classifier model to file "%s"\' % classifier_filename_exp)\n-                \n-            elif (args.mode==\'CLASSIFY\'):\n-                # Classify images\n-                print(\'Testing classifier\')\n-                with open(classifier_filename_exp, \'rb\') as infile:\n-                    (model, class_names) = pickle.load(infile)\n-\n-                print(\'Loaded classifier model from file "%s"\' % classifier_filename_exp)\n-\n-                predictions = model.predict_proba(emb_array)\n-                best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                \n-                for i in range(len(best_class_indices)):\n-                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n-                    \n-                accuracy = np.mean(np.equal(best_class_indices, labels))\n-                print(\'Accuracy: %.3f\' % accuracy)\n-                \n-            \n-def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n-    train_set = []\n-    test_set = []\n-    for cls in dataset:\n-        paths = cls.image_paths\n-        # Remove classes with less than min_nrof_images_per_class\n-        if len(paths)>=min_nrof_images_per_class:\n-            np.random.shuffle(paths)\n-            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n-            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n-    return train_set, test_set\n-\n-            \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'mode\', type=str, choices=[\'TRAIN\', \'CLASSIFY\'],\n-        help=\'Indicates if a new classifier should be trained or a classification \' + \n-        \'model should be used for classification\', default=\'CLASSIFY\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'classifier_filename\', \n-        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n-        \'For training this is the output and for classification this is an input.\')\n-    parser.add_argument(\'--use_split_dataset\', \n-        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +  \n-        \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n-    parser.add_argument(\'--test_data_dir\', type=str,\n-        help=\'Path to the test data directory containing aligned images used for testing.\')\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--min_nrof_images_per_class\', type=int,\n-        help=\'Only include classes with at least this number of images in the dataset\', default=20)\n-    parser.add_argument(\'--nrof_train_images_per_class\', type=int,\n-        help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n-    \n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/compare.py b/src/compare.py\ndeleted file mode 100644\nindex bc53cc4..0000000\n--- a/src/compare.py\n+++ /dev/null\n@@ -1,130 +0,0 @@\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import tensorflow as tf\n-import numpy as np\n-import sys\n-import os\n-import copy\n-import argparse\n-import facenet\n-import align.detect_face\n-\n-def main(args):\n-\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n-    with tf.Graph().as_default():\n-\n-        with tf.Session() as sess:\n-      \n-            # Load the model\n-            facenet.load_model(args.model)\n-    \n-            # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-\n-            # Run forward pass to calculate embeddings\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            nrof_images = len(args.image_files)\n-\n-            print(\'Images:\')\n-            for i in range(nrof_images):\n-                print(\'%1d: %s\' % (i, args.image_files[i]))\n-            print(\'\')\n-            \n-            # Print distance matrix\n-            print(\'Distance matrix\')\n-            print(\'    \', end=\'\')\n-            for i in range(nrof_images):\n-                print(\'    %1d     \' % i, end=\'\')\n-            print(\'\')\n-            for i in range(nrof_images):\n-                print(\'%1d  \' % i, end=\'\')\n-                for j in range(nrof_images):\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\n-                    print(\'  %1.4f  \' % dist, end=\'\')\n-                print(\'\')\n-            \n-            \n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n-\n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-    \n-    print(\'Creating networks and loading parameters\')\n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-  \n-    tmp_image_paths=copy.copy(image_paths)\n-    img_list = []\n-    for image in tmp_image_paths:\n-        img = misc.imread(os.path.expanduser(image), mode=\'RGB\')\n-        img_size = np.asarray(img.shape)[0:2]\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-        if len(bounding_boxes) < 1:\n-          image_paths.remove(image)\n-          print("can\'t detect face, remove ", image)\n-          continue\n-        det = np.squeeze(bounding_boxes[0,0:4])\n-        bb = np.zeros(4, dtype=np.int32)\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n-        prewhitened = facenet.prewhiten(aligned)\n-        img_list.append(prewhitened)\n-    images = np.stack(img_list)\n-    return images\n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'image_files\', type=str, nargs=\'+\', help=\'Images to compare\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\ndeleted file mode 100644\nindex 4556bfa..0000000\n--- a/src/decode_msceleb_dataset.py\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import numpy as np\n-import base64\n-import sys\n-import os\n-import cv2\n-import argparse\n-import facenet\n-\n-\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\n-# Column1: Freebase MID\n-# Column2: Query/Name\n-# Column3: ImageSearchRank\n-# Column4: ImageURL\n-# Column5: PageURL\n-# Column6: ImageData_Base64Encoded\n-\n-def main(args):\n-    output_dir = os.path.expanduser(args.output_dir)\n-  \n-    if not os.path.exists(output_dir):\n-        os.mkdir(output_dir)\n-  \n-    # Store some git revision info in a text file in the output directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    \n-    i = 0\n-    for f in args.tsv_files:\n-        for line in f:\n-            fields = line.split(\'\\t\')\n-            class_dir = fields[0]\n-            img_name = fields[1] + \'-\' + fields[4] + \'.\' + args.output_format\n-            img_string = fields[5]\n-            img_dec_string = base64.b64decode(img_string)\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\n-            if args.size:\n-                img = misc.imresize(img, (args.size, args.size), interp=\'bilinear\')\n-            full_class_dir = os.path.join(output_dir, class_dir)\n-            if not os.path.exists(full_class_dir):\n-                os.mkdir(full_class_dir)\n-            full_path = os.path.join(full_class_dir, img_name.replace(\'/\',\'_\'))\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\n-            print(\'%8d: %s\' % (i, full_path))\n-            i += 1\n-  \n-if __name__ == \'__main__\':\n-    parser = argparse.ArgumentParser()\n-\n-    parser.add_argument(\'output_dir\', type=str, help=\'Output base directory for the image dataset\')\n-    parser.add_argument(\'tsv_files\', type=argparse.FileType(\'r\'), nargs=\'+\', help=\'Input TSV file name(s)\')\n-    parser.add_argument(\'--size\', type=int, help=\'Images are resized to the given size\')\n-    parser.add_argument(\'--output_format\', type=str, help=\'Format of the output images\', default=\'png\', choices=[\'png\', \'jpg\'])\n-\n-    main(parser.parse_args())\n-\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\ndeleted file mode 100644\nindex a835ac2..0000000\n--- a/src/download_and_extract.py\n+++ /dev/null\n@@ -1,51 +0,0 @@\n-import requests\n-import zipfile\n-import os\n-\n-model_dict = {\n-    \'lfw-subset\':      \'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\', \n-    \'20170131-234652\': \'0B5MzpY9kBtDVSGM0RmVET2EwVEk\',\n-    \'20170216-091149\': \'0B5MzpY9kBtDVTGZjcWkzT3pldDA\',\n-    \'20170512-110547\': \'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\',\n-    \'20180402-114759\': \'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\'\n-    }\n-\n-def download_and_extract_file(model_name, data_dir):\n-    file_id = model_dict[model_name]\n-    destination = os.path.join(data_dir, model_name + \'.zip\')\n-    if not os.path.exists(destination):\n-        print(\'Downloading file to %s\' % destination)\n-        download_file_from_google_drive(file_id, destination)\n-        with zipfile.ZipFile(destination, \'r\') as zip_ref:\n-            print(\'Extracting file to %s\' % data_dir)\n-            zip_ref.extractall(data_dir)\n-\n-def download_file_from_google_drive(file_id, destination):\n-    \n-        URL = "https://drive.google.com/uc?export=download"\n-    \n-        session = requests.Session()\n-    \n-        response = session.get(URL, params = { \'id\' : file_id }, stream = True)\n-        token = get_confirm_token(response)\n-    \n-        if token:\n-            params = { \'id\' : file_id, \'confirm\' : token }\n-            response = session.get(URL, params = params, stream = True)\n-    \n-        save_response_content(response, destination)    \n-\n-def get_confirm_token(response):\n-    for key, value in response.cookies.items():\n-        if key.startswith(\'download_warning\'):\n-            return value\n-\n-    return None\n-\n-def save_response_content(response, destination):\n-    CHUNK_SIZE = 32768\n-\n-    with open(destination, "wb") as f:\n-        for chunk in response.iter_content(CHUNK_SIZE):\n-            if chunk: # filter out keep-alive new chunks\n-                f.write(chunk)\ndiff --git a/src/face_rec.py b/src/face_rec.py\ndeleted file mode 100644\nindex 1eb578d..0000000\n--- a/src/face_rec.py\n+++ /dev/null\n@@ -1,135 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n-    args = parser.parse_args()\n-    \n-    # Cai dat cac tham so can thiet\n-    MINSIZE = 20\n-    THRESHOLD = [0.6, 0.7, 0.7]\n-    FACTOR = 0.709\n-    IMAGE_SIZE = 182\n-    INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n-    VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n-\n-    # Load model da train de nhan dien khuon mat - thuc chat la classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n-        model, class_names = pickle.load(file)\n-    print("Custom Classifier, Successfully loaded")\n-\n-    with tf.Graph().as_default():\n-\n-        # Cai dat GPU neu co\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-        with sess.as_default():\n-\n-            # Load model MTCNN phat hien khuon mat\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(FACENET_MODEL_PATH)\n-\n-            # Lay tensor input va output\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-\n-            # Cai dat cac mang con\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n-\n-            people_detected = set()\n-            person_detected = collections.Counter()\n-\n-            # Lay hinh anh tu file video\n-            cap = cv2.VideoCapture(VIDEO_PATH)\n-\n-            while (cap.isOpened()):\n-                # Doc tung frame\n-                ret, frame = cap.read()\n-\n-                # Phat hien khuon mat, tra ve vi tri trong bounding_boxes\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-                faces_found = bounding_boxes.shape[0]\n-                try:\n-                    # Neu co it nhat 1 khuon mat trong frame\n-                    if faces_found > 0:\n-                        det = bounding_boxes[:, 0:4]\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\n-                        for i in range(faces_found):\n-                            bb[i][0] = det[i][0]\n-                            bb[i][1] = det[i][1]\n-                            bb[i][2] = det[i][2]\n-                            bb[i][3] = det[i][3]\n-\n-                            # Cat phan khuon mat tim duoc\n-                            cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                            scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                interpolation=cv2.INTER_CUBIC)\n-                            scaled = facenet.prewhiten(scaled)\n-                            scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                            feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                            emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-                            \n-                            # Dua vao model de classifier\n-                            predictions = model.predict_proba(emb_array)\n-                            best_class_indices = np.argmax(predictions, axis=1)\n-                            best_class_probabilities = predictions[\n-                                np.arange(len(best_class_indices)), best_class_indices]\n-                            \n-                            # Lay ra ten va ty le % cua class co ty le cao nhat\n-                            best_name = class_names[best_class_indices[0]]\n-                            print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-                            # Ve khung mau xanh quanh khuon mat\n-                            cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                            text_x = bb[i][0]\n-                            text_y = bb[i][3] + 20\n-\n-                            # Neu ty le nhan dang > 0.5 thi hien thi ten\n-                            if best_class_probabilities > 0.5:\n-                                name = class_names[best_class_indices[0]]\n-                            else:\n-                                # Con neu <=0.5 thi hien thi Unknow\n-                                name = "Unknown"\n-                                \n-                            # Viet text len tren frame    \n-                            cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\n-                            cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                        cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                        1, (255, 255, 255), thickness=1, lineType=2)\n-                            person_detected[best_name] += 1\n-                except:\n-                    pass\n-\n-                # Hien thi frame len man hinh\n-                cv2.imshow(\'Face Recognition\', frame)\n-                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                    break\n-\n-            cap.release()\n-            cv2.destroyAllWindows()\n-\n-\n-main()\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\ndeleted file mode 100644\nindex cfbd4f4..0000000\n--- a/src/face_rec_cam.py\n+++ /dev/null\n@@ -1,133 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-from imutils.video import VideoStream\n-\n-\n-import argparse\n-import facenet\n-import imutils\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\'--path\', help=\'Path of the video you want to test on.\', default=0)\n-    args = parser.parse_args()\n-\n-    MINSIZE = 20\n-    THRESHOLD = [0.6, 0.7, 0.7]\n-    FACTOR = 0.709\n-    IMAGE_SIZE = 182\n-    INPUT_IMAGE_SIZE = 160\n-    CLASSIFIER_PATH = \'Models/facemodel.pkl\'\n-    VIDEO_PATH = args.path\n-    FACENET_MODEL_PATH = \'Models/20180402-114759.pb\'\n-\n-    # Load The Custom Classifier\n-    with open(CLASSIFIER_PATH, \'rb\') as file:\n-        model, class_names = pickle.load(file)\n-    print("Custom Classifier, Successfully loaded")\n-\n-    with tf.Graph().as_default():\n-\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-        with sess.as_default():\n-\n-            # Load the model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(FACENET_MODEL_PATH)\n-\n-            # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\n-\n-            people_detected = set()\n-            person_detected = collections.Counter()\n-\n-            cap  = VideoStream(src=0).start()\n-\n-            while (True):\n-                frame = cap.read()\n-                frame = imutils.resize(frame, width=600)\n-                frame = cv2.flip(frame, 1)\n-\n-                bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-                faces_found = bounding_boxes.shape[0]\n-                try:\n-                    if faces_found > 1:\n-                        cv2.putText(frame, "Only one face", (0, 100), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                    1, (255, 255, 255), thickness=1, lineType=2)\n-                    elif faces_found > 0:\n-                        det = bounding_boxes[:, 0:4]\n-                        bb = np.zeros((faces_found, 4), dtype=np.int32)\n-                        for i in range(faces_found):\n-                            bb[i][0] = det[i][0]\n-                            bb[i][1] = det[i][1]\n-                            bb[i][2] = det[i][2]\n-                            bb[i][3] = det[i][3]\n-                            print(bb[i][3]-bb[i][1])\n-                            print(frame.shape[0])\n-                            print((bb[i][3]-bb[i][1])/frame.shape[0])\n-                            if (bb[i][3]-bb[i][1])/frame.shape[0]>0.25:\n-                                cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                                    interpolation=cv2.INTER_CUBIC)\n-                                scaled = facenet.prewhiten(scaled)\n-                                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-\n-                                predictions = model.predict_proba(emb_array)\n-                                best_class_indices = np.argmax(predictions, axis=1)\n-                                best_class_probabilities = predictions[\n-                                    np.arange(len(best_class_indices)), best_class_indices]\n-                                best_name = class_names[best_class_indices[0]]\n-                                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-\n-\n-                                if best_class_probabilities > 0.8:\n-                                    cv2.rectangle(frame, (bb[i][0], bb[i][1]), (bb[i][2], bb[i][3]), (0, 255, 0), 2)\n-                                    text_x = bb[i][0]\n-                                    text_y = bb[i][3] + 20\n-\n-                                    name = class_names[best_class_indices[0]]\n-                                    cv2.putText(frame, name, (text_x, text_y), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    cv2.putText(frame, str(round(best_class_probabilities[0], 3)), (text_x, text_y + 17),\n-                                                cv2.FONT_HERSHEY_COMPLEX_SMALL,\n-                                                1, (255, 255, 255), thickness=1, lineType=2)\n-                                    person_detected[best_name] += 1\n-                                else:\n-                                    name = "Unknown"\n-\n-                except:\n-                    pass\n-\n-                cv2.imshow(\'Face Recognition\', frame)\n-                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n-                    break\n-\n-            cap.release()\n-            cv2.destroyAllWindows()\n-\n-\n-main()\n\\ No newline at end of file\ndiff --git a/src/face_rec_flask.py b/src/face_rec_flask.py\ndeleted file mode 100644\nindex 16868f2..0000000\n--- a/src/face_rec_flask.py\n+++ /dev/null\n@@ -1,117 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from flask import Flask\n-from flask import render_template , request\n-from flask_cors import CORS, cross_origin\n-import tensorflow as tf\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-import align.detect_face\n-import numpy as np\n-import cv2\n-import collections\n-from sklearn.svm import SVC\n-import base64\n-\n-MINSIZE = 20\n-THRESHOLD = [0.6, 0.7, 0.7]\n-FACTOR = 0.709\n-IMAGE_SIZE = 182\n-INPUT_IMAGE_SIZE = 160\n-CLASSIFIER_PATH = \'../Models/facemodel.pkl\'\n-FACENET_MODEL_PATH = \'../Models/20180402-114759.pb\'\n-\n-# Load The Custom Classifier\n-with open(CLASSIFIER_PATH, \'rb\') as file:\n-    model, class_names = pickle.load(file)\n-print("Custom Classifier, Successfully loaded")\n-\n-tf.Graph().as_default()\n-\n-gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n-sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-\n-\n-# Load the model\n-print(\'Loading feature extraction model\')\n-facenet.load_model(FACENET_MODEL_PATH)\n-\n-# Get input and output tensors\n-images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-embedding_size = embeddings.get_shape()[1]\n-pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "align")\n-\n-\n-\n-app = Flask(__name__)\n-CORS(app)\n-\n-\n-\n-@app.route(\'/\')\n-@cross_origin()\n-def index():\n-    return "OK!";\n-\n-@app.route(\'/recog\', methods=[\'POST\'])\n-@cross_origin()\n-def upload_img_file():\n-    if request.method == \'POST\':\n-        # base 64\n-        name="Unknown"\n-        f = request.form.get(\'image\')\n-        w = int(request.form.get(\'w\'))\n-        h = int(request.form.get(\'h\'))\n-\n-        decoded_string = base64.b64decode(f)\n-        frame = np.fromstring(decoded_string, dtype=np.uint8)\n-        #frame = frame.reshape(w,h,3)\n-        frame = cv2.imdecode(frame, cv2.IMREAD_ANYCOLOR)  # cv2.IMREAD_COLOR in OpenCV 3.1\n-\n-        bounding_boxes, _ = align.detect_face.detect_face(frame, MINSIZE, pnet, rnet, onet, THRESHOLD, FACTOR)\n-\n-        faces_found = bounding_boxes.shape[0]\n-\n-        if faces_found > 0:\n-            det = bounding_boxes[:, 0:4]\n-            bb = np.zeros((faces_found, 4), dtype=np.int32)\n-            for i in range(faces_found):\n-                bb[i][0] = det[i][0]\n-                bb[i][1] = det[i][1]\n-                bb[i][2] = det[i][2]\n-                bb[i][3] = det[i][3]\n-                cropped = frame\n-                #cropped = frame[bb[i][1]:bb[i][3], bb[i][0]:bb[i][2], :]\n-                scaled = cv2.resize(cropped, (INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE),\n-                                    interpolation=cv2.INTER_CUBIC)\n-                scaled = facenet.prewhiten(scaled)\n-                scaled_reshape = scaled.reshape(-1, INPUT_IMAGE_SIZE, INPUT_IMAGE_SIZE, 3)\n-                feed_dict = {images_placeholder: scaled_reshape, phase_train_placeholder: False}\n-                emb_array = sess.run(embeddings, feed_dict=feed_dict)\n-                predictions = model.predict_proba(emb_array)\n-                best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[\n-                    np.arange(len(best_class_indices)), best_class_indices]\n-                best_name = class_names[best_class_indices[0]]\n-                print("Name: {}, Probability: {}".format(best_name, best_class_probabilities))\n-\n-                if best_class_probabilities > 0.8:\n-                    name = class_names[best_class_indices[0]]\n-                else:\n-                    name = "Unknown"\n-\n-\n-        return name;\n-\n-\n-if __name__ == \'__main__\':\n-    app.run(debug=True, host=\'0.0.0.0\',port=\'8000\')\n-\ndiff --git a/src/facenet.py b/src/facenet.py\ndeleted file mode 100644\nindex bfe6802..0000000\n--- a/src/facenet.py\n+++ /dev/null\n@@ -1,571 +0,0 @@\n-"""Functions for building the face recognition network.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.p\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-# pylint: disable=missing-docstring\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import os\n-from subprocess import Popen, PIPE\n-import tensorflow as tf\n-import numpy as np\n-from scipy import misc\n-from sklearn.model_selection import KFold\n-from scipy import interpolate\n-from tensorflow.python.training import training\n-import random\n-import re\n-from tensorflow.python.platform import gfile\n-import math\n-from six import iteritems\n-\n-def triplet_loss(anchor, positive, negative, alpha):\n-    """Calculate the triplet loss according to the FaceNet paper\n-    \n-    Args:\n-      anchor: the embeddings for the anchor images.\n-      positive: the embeddings for the positive images.\n-      negative: the embeddings for the negative images.\n-  \n-    Returns:\n-      the triplet loss according to the FaceNet paper as a float tensor.\n-    """\n-    with tf.variable_scope(\'triplet_loss\'):\n-        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n-        neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n-        \n-        basic_loss = tf.add(tf.subtract(pos_dist,neg_dist), alpha)\n-        loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n-      \n-    return loss\n-  \n-def center_loss(features, label, alfa, nrof_classes):\n-    """Center loss based on the paper "A Discriminative Feature Learning Approach for Deep Face Recognition"\n-       (http://ydwen.github.io/papers/WenECCV16.pdf)\n-    """\n-    nrof_features = features.get_shape()[1]\n-    centers = tf.get_variable(\'centers\', [nrof_classes, nrof_features], dtype=tf.float32,\n-        initializer=tf.constant_initializer(0), trainable=False)\n-    label = tf.reshape(label, [-1])\n-    centers_batch = tf.gather(centers, label)\n-    diff = (1 - alfa) * (centers_batch - features)\n-    centers = tf.scatter_sub(centers, label, diff)\n-    with tf.control_dependencies([centers]):\n-        loss = tf.reduce_mean(tf.square(features - centers_batch))\n-    return loss, centers\n-\n-def get_image_paths_and_labels(dataset):\n-    image_paths_flat = []\n-    labels_flat = []\n-    for i in range(len(dataset)):\n-        image_paths_flat += dataset[i].image_paths\n-        labels_flat += [i] * len(dataset[i].image_paths)\n-    return image_paths_flat, labels_flat\n-\n-def shuffle_examples(image_paths, labels):\n-    shuffle_list = list(zip(image_paths, labels))\n-    random.shuffle(shuffle_list)\n-    image_paths_shuff, labels_shuff = zip(*shuffle_list)\n-    return image_paths_shuff, labels_shuff\n-\n-def random_rotate_image(image):\n-    angle = np.random.uniform(low=-10.0, high=10.0)\n-    return misc.imrotate(image, angle, \'bicubic\')\n-  \n-# 1: Random rotate 2: Random crop  4: Random flip  8:  Fixed image standardization  16: Flip\n-RANDOM_ROTATE = 1\n-RANDOM_CROP = 2\n-RANDOM_FLIP = 4\n-FIXED_STANDARDIZATION = 8\n-FLIP = 16\n-def create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder):\n-    images_and_labels_list = []\n-    for _ in range(nrof_preprocess_threads):\n-        filenames, label, control = input_queue.dequeue()\n-        images = []\n-        for filename in tf.unstack(filenames):\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, 3)\n-            image = tf.cond(get_control_flag(control[0], RANDOM_ROTATE),\n-                            lambda:tf.py_func(random_rotate_image, [image], tf.uint8), \n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_CROP), \n-                            lambda:tf.random_crop(image, image_size + (3,)), \n-                            lambda:tf.image.resize_image_with_crop_or_pad(image, image_size[0], image_size[1]))\n-            image = tf.cond(get_control_flag(control[0], RANDOM_FLIP),\n-                            lambda:tf.image.random_flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            image = tf.cond(get_control_flag(control[0], FIXED_STANDARDIZATION),\n-                            lambda:(tf.cast(image, tf.float32) - 127.5)/128.0,\n-                            lambda:tf.image.per_image_standardization(image))\n-            image = tf.cond(get_control_flag(control[0], FLIP),\n-                            lambda:tf.image.flip_left_right(image),\n-                            lambda:tf.identity(image))\n-            #pylint: disable=no-member\n-            image.set_shape(image_size + (3,))\n-            images.append(image)\n-        images_and_labels_list.append([images, label])\n-\n-    image_batch, label_batch = tf.train.batch_join(\n-        images_and_labels_list, batch_size=batch_size_placeholder, \n-        shapes=[image_size + (3,), ()], enqueue_many=True,\n-        capacity=4 * nrof_preprocess_threads * 100,\n-        allow_smaller_final_batch=True)\n-    \n-    return image_batch, label_batch\n-\n-def get_control_flag(control, field):\n-    return tf.equal(tf.mod(tf.floor_div(control, field), 2), 1)\n-  \n-def _add_loss_summaries(total_loss):\n-    """Add summaries for losses.\n-  \n-    Generates moving average for all losses and associated summaries for\n-    visualizing the performance of the network.\n-  \n-    Args:\n-      total_loss: Total loss from loss().\n-    Returns:\n-      loss_averages_op: op for generating moving averages of losses.\n-    """\n-    # Compute the moving average of all individual losses and the total loss.\n-    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n-    losses = tf.get_collection(\'losses\')\n-    loss_averages_op = loss_averages.apply(losses + [total_loss])\n-  \n-    # Attach a scalar summmary to all individual losses and the total loss; do the\n-    # same for the averaged version of the losses.\n-    for l in losses + [total_loss]:\n-        # Name each loss as \'(raw)\' and name the moving average version of the loss\n-        # as the original loss name.\n-        tf.summary.scalar(l.op.name +\' (raw)\', l)\n-        tf.summary.scalar(l.op.name, loss_averages.average(l))\n-  \n-    return loss_averages_op\n-\n-def train(total_loss, global_step, optimizer, learning_rate, moving_average_decay, update_gradient_vars, log_histograms=True):\n-    # Generate moving averages of all losses and associated summaries.\n-    loss_averages_op = _add_loss_summaries(total_loss)\n-\n-    # Compute gradients.\n-    with tf.control_dependencies([loss_averages_op]):\n-        if optimizer==\'ADAGRAD\':\n-            opt = tf.train.AdagradOptimizer(learning_rate)\n-        elif optimizer==\'ADADELTA\':\n-            opt = tf.train.AdadeltaOptimizer(learning_rate, rho=0.9, epsilon=1e-6)\n-        elif optimizer==\'ADAM\':\n-            opt = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999, epsilon=0.1)\n-        elif optimizer==\'RMSPROP\':\n-            opt = tf.train.RMSPropOptimizer(learning_rate, decay=0.9, momentum=0.9, epsilon=1.0)\n-        elif optimizer==\'MOM\':\n-            opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_nesterov=True)\n-        else:\n-            raise ValueError(\'Invalid optimization algorithm\')\n-    \n-        grads = opt.compute_gradients(total_loss, update_gradient_vars)\n-        \n-    # Apply gradients.\n-    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n-  \n-    # Add histograms for trainable variables.\n-    if log_histograms:\n-        for var in tf.trainable_variables():\n-            tf.summary.histogram(var.op.name, var)\n-   \n-    # Add histograms for gradients.\n-    if log_histograms:\n-        for grad, var in grads:\n-            if grad is not None:\n-                tf.summary.histogram(var.op.name + \'/gradients\', grad)\n-  \n-    # Track the moving averages of all trainable variables.\n-    variable_averages = tf.train.ExponentialMovingAverage(\n-        moving_average_decay, global_step)\n-    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n-  \n-    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n-        train_op = tf.no_op(name=\'train\')\n-  \n-    return train_op\n-\n-def prewhiten(x):\n-    mean = np.mean(x)\n-    std = np.std(x)\n-    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n-    y = np.multiply(np.subtract(x, mean), 1/std_adj)\n-    return y  \n-\n-def crop(image, random_crop, image_size):\n-    if image.shape[1]>image_size:\n-        sz1 = int(image.shape[1]//2)\n-        sz2 = int(image_size//2)\n-        if random_crop:\n-            diff = sz1-sz2\n-            (h, v) = (np.random.randint(-diff, diff+1), np.random.randint(-diff, diff+1))\n-        else:\n-            (h, v) = (0,0)\n-        image = image[(sz1-sz2+v):(sz1+sz2+v),(sz1-sz2+h):(sz1+sz2+h),:]\n-    return image\n-  \n-def flip(image, random_flip):\n-    if random_flip and np.random.choice([True, False]):\n-        image = np.fliplr(image)\n-    return image\n-\n-def to_rgb(img):\n-    w, h = img.shape\n-    ret = np.empty((w, h, 3), dtype=np.uint8)\n-    ret[:, :, 0] = ret[:, :, 1] = ret[:, :, 2] = img\n-    return ret\n-  \n-def load_data(image_paths, do_random_crop, do_random_flip, image_size, do_prewhiten=True):\n-    nrof_samples = len(image_paths)\n-    images = np.zeros((nrof_samples, image_size, image_size, 3))\n-    for i in range(nrof_samples):\n-        img = misc.imread(image_paths[i])\n-        if img.ndim == 2:\n-            img = to_rgb(img)\n-        if do_prewhiten:\n-            img = prewhiten(img)\n-        img = crop(img, do_random_crop, image_size)\n-        img = flip(img, do_random_flip)\n-        images[i,:,:,:] = img\n-    return images\n-\n-def get_label_batch(label_data, batch_size, batch_index):\n-    nrof_examples = np.size(label_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = label_data[j:j+batch_size]\n-    else:\n-        x1 = label_data[j:nrof_examples]\n-        x2 = label_data[0:nrof_examples-j]\n-        batch = np.vstack([x1,x2])\n-    batch_int = batch.astype(np.int64)\n-    return batch_int\n-\n-def get_batch(image_data, batch_size, batch_index):\n-    nrof_examples = np.size(image_data, 0)\n-    j = batch_index*batch_size % nrof_examples\n-    if j+batch_size<=nrof_examples:\n-        batch = image_data[j:j+batch_size,:,:,:]\n-    else:\n-        x1 = image_data[j:nrof_examples,:,:,:]\n-        x2 = image_data[0:nrof_examples-j,:,:,:]\n-        batch = np.vstack([x1,x2])\n-    batch_float = batch.astype(np.float32)\n-    return batch_float\n-\n-def get_triplet_batch(triplets, batch_index, batch_size):\n-    ax, px, nx = triplets\n-    a = get_batch(ax, int(batch_size/3), batch_index)\n-    p = get_batch(px, int(batch_size/3), batch_index)\n-    n = get_batch(nx, int(batch_size/3), batch_index)\n-    batch = np.vstack([a, p, n])\n-    return batch\n-\n-def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                if par[1]==\'-\':\n-                    lr = -1\n-                else:\n-                    lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n-\n-class ImageClass():\n-    "Stores the paths to images for a given class"\n-    def __init__(self, name, image_paths):\n-        self.name = name\n-        self.image_paths = image_paths\n-  \n-    def __str__(self):\n-        return self.name + \', \' + str(len(self.image_paths)) + \' images\'\n-  \n-    def __len__(self):\n-        return len(self.image_paths)\n-  \n-def get_dataset(path, has_class_directories=True):\n-    dataset = []\n-    path_exp = os.path.expanduser(path)\n-    classes = [path for path in os.listdir(path_exp) \\\n-                    if os.path.isdir(os.path.join(path_exp, path))]\n-    classes.sort()\n-    nrof_classes = len(classes)\n-    for i in range(nrof_classes):\n-        class_name = classes[i]\n-        facedir = os.path.join(path_exp, class_name)\n-        image_paths = get_image_paths(facedir)\n-        dataset.append(ImageClass(class_name, image_paths))\n-  \n-    return dataset\n-\n-def get_image_paths(facedir):\n-    image_paths = []\n-    if os.path.isdir(facedir):\n-        images = os.listdir(facedir)\n-        image_paths = [os.path.join(facedir,img) for img in images]\n-    return image_paths\n-  \n-def split_dataset(dataset, split_ratio, min_nrof_images_per_class, mode):\n-    if mode==\'SPLIT_CLASSES\':\n-        nrof_classes = len(dataset)\n-        class_indices = np.arange(nrof_classes)\n-        np.random.shuffle(class_indices)\n-        split = int(round(nrof_classes*(1-split_ratio)))\n-        train_set = [dataset[i] for i in class_indices[0:split]]\n-        test_set = [dataset[i] for i in class_indices[split:-1]]\n-    elif mode==\'SPLIT_IMAGES\':\n-        train_set = []\n-        test_set = []\n-        for cls in dataset:\n-            paths = cls.image_paths\n-            np.random.shuffle(paths)\n-            nrof_images_in_class = len(paths)\n-            split = int(math.floor(nrof_images_in_class*(1-split_ratio)))\n-            if split==nrof_images_in_class:\n-                split = nrof_images_in_class-1\n-            if split>=min_nrof_images_per_class and nrof_images_in_class-split>=1:\n-                train_set.append(ImageClass(cls.name, paths[:split]))\n-                test_set.append(ImageClass(cls.name, paths[split:]))\n-    else:\n-        raise ValueError(\'Invalid train/test split mode "%s"\' % mode)\n-    return train_set, test_set\n-\n-def load_model(model, input_map=None):\n-    # Check if the model is a model directory (containing a metagraph and a checkpoint file)\n-    #  or if it is a protobuf file with a frozen graph\n-    model_exp = os.path.expanduser(model)\n-    if (os.path.isfile(model_exp)):\n-        print(\'Model filename: %s\' % model_exp)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'\')\n-    else:\n-        print(\'Model directory: %s\' % model_exp)\n-        meta_file, ckpt_file = get_model_filenames(model_exp)\n-        \n-        print(\'Metagraph file: %s\' % meta_file)\n-        print(\'Checkpoint file: %s\' % ckpt_file)\n-      \n-        saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file), input_map=input_map)\n-        saver.restore(tf.get_default_session(), os.path.join(model_exp, ckpt_file))\n-    \n-def get_model_filenames(model_dir):\n-    files = os.listdir(model_dir)\n-    meta_files = [s for s in files if s.endswith(\'.meta\')]\n-    if len(meta_files)==0:\n-        raise ValueError(\'No meta file found in the model directory (%s)\' % model_dir)\n-    elif len(meta_files)>1:\n-        raise ValueError(\'There should not be more than one meta file in the model directory (%s)\' % model_dir)\n-    meta_file = meta_files[0]\n-    ckpt = tf.train.get_checkpoint_state(model_dir)\n-    if ckpt and ckpt.model_checkpoint_path:\n-        ckpt_file = os.path.basename(ckpt.model_checkpoint_path)\n-        return meta_file, ckpt_file\n-\n-    meta_files = [s for s in files if \'.ckpt\' in s]\n-    max_step = -1\n-    for f in files:\n-        step_str = re.match(r\'(^model-[\\w\\- ]+.ckpt-(\\d+))\', f)\n-        if step_str is not None and len(step_str.groups())>=2:\n-            step = int(step_str.groups()[1])\n-            if step > max_step:\n-                max_step = step\n-                ckpt_file = step_str.groups()[0]\n-    return meta_file, ckpt_file\n-  \n-def distance(embeddings1, embeddings2, distance_metric=0):\n-    if distance_metric==0:\n-        # Euclidian distance\n-        diff = np.subtract(embeddings1, embeddings2)\n-        dist = np.sum(np.square(diff),1)\n-    elif distance_metric==1:\n-        # Distance based on cosine similarity\n-        dot = np.sum(np.multiply(embeddings1, embeddings2), axis=1)\n-        norm = np.linalg.norm(embeddings1, axis=1) * np.linalg.norm(embeddings2, axis=1)\n-        similarity = dot / norm\n-        dist = np.arccos(similarity) / math.pi\n-    else:\n-        raise \'Undefined distance metric %d\' % distance_metric \n-        \n-    return dist\n-\n-def calculate_roc(thresholds, embeddings1, embeddings2, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    tprs = np.zeros((nrof_folds,nrof_thresholds))\n-    fprs = np.zeros((nrof_folds,nrof_thresholds))\n-    accuracy = np.zeros((nrof_folds))\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-        \n-        # Find the best threshold for the fold\n-        acc_train = np.zeros((nrof_thresholds))\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, _, acc_train[threshold_idx] = calculate_accuracy(threshold, dist[train_set], actual_issame[train_set])\n-        best_threshold_index = np.argmax(acc_train)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            tprs[fold_idx,threshold_idx], fprs[fold_idx,threshold_idx], _ = calculate_accuracy(threshold, dist[test_set], actual_issame[test_set])\n-        _, _, accuracy[fold_idx] = calculate_accuracy(thresholds[best_threshold_index], dist[test_set], actual_issame[test_set])\n-          \n-        tpr = np.mean(tprs,0)\n-        fpr = np.mean(fprs,0)\n-    return tpr, fpr, accuracy\n-\n-def calculate_accuracy(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n-    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n-    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n-  \n-    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n-    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n-    acc = float(tp+tn)/dist.size\n-    return tpr, fpr, acc\n-\n-\n-  \n-def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    assert(embeddings1.shape[0] == embeddings2.shape[0])\n-    assert(embeddings1.shape[1] == embeddings2.shape[1])\n-    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n-    nrof_thresholds = len(thresholds)\n-    k_fold = KFold(n_splits=nrof_folds, shuffle=False)\n-    \n-    val = np.zeros(nrof_folds)\n-    far = np.zeros(nrof_folds)\n-    \n-    indices = np.arange(nrof_pairs)\n-    \n-    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n-        if subtract_mean:\n-            mean = np.mean(np.concatenate([embeddings1[train_set], embeddings2[train_set]]), axis=0)\n-        else:\n-          mean = 0.0\n-        dist = distance(embeddings1-mean, embeddings2-mean, distance_metric)\n-      \n-        # Find the threshold that gives FAR = far_target\n-        far_train = np.zeros(nrof_thresholds)\n-        for threshold_idx, threshold in enumerate(thresholds):\n-            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set])\n-        if np.max(far_train)>=far_target:\n-            f = interpolate.interp1d(far_train, thresholds, kind=\'slinear\')\n-            threshold = f(far_target)\n-        else:\n-            threshold = 0.0\n-    \n-        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set])\n-  \n-    val_mean = np.mean(val)\n-    far_mean = np.mean(far)\n-    val_std = np.std(val)\n-    return val_mean, val_std, far_mean\n-\n-\n-def calculate_val_far(threshold, dist, actual_issame):\n-    predict_issame = np.less(dist, threshold)\n-    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n-    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n-    n_same = np.sum(actual_issame)\n-    n_diff = np.sum(np.logical_not(actual_issame))\n-    val = float(true_accept) / float(n_same)\n-    far = float(false_accept) / float(n_diff)\n-    return val, far\n-\n-def store_revision_info(src_path, output_dir, arg_string):\n-    try:\n-        # Get git hash\n-        cmd = [\'git\', \'rev-parse\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_hash = stdout.strip()\n-    except OSError as e:\n-        git_hash = \' \'.join(cmd) + \': \' +  e.strerror\n-  \n-    try:\n-        # Get local changes\n-        cmd = [\'git\', \'diff\', \'HEAD\']\n-        gitproc = Popen(cmd, stdout = PIPE, cwd=src_path)\n-        (stdout, _) = gitproc.communicate()\n-        git_diff = stdout.strip()\n-    except OSError as e:\n-        git_diff = \' \'.join(cmd) + \': \' +  e.strerror\n-    \n-    # Store a text file in the log directory\n-    rev_info_filename = os.path.join(output_dir, \'revision_info.txt\')\n-    with open(rev_info_filename, "w") as text_file:\n-        text_file.write(\'arguments: %s\\n--------------------\\n\' % arg_string)\n-        text_file.write(\'tensorflow version: %s\\n--------------------\\n\' % tf.__version__)  # @UndefinedVariable\n-        text_file.write(\'git hash: %s\\n--------------------\\n\' % git_hash)\n-        text_file.write(\'%s\' % git_diff)\n-\n-def list_variables(filename):\n-    reader = training.NewCheckpointReader(filename)\n-    variable_map = reader.get_variable_to_shape_map()\n-    names = sorted(variable_map.keys())\n-    return names\n-\n-def put_images_on_grid(images, shape=(16,8)):\n-    nrof_images = images.shape[0]\n-    img_size = images.shape[1]\n-    bw = 3\n-    img = np.zeros((shape[1]*(img_size+bw)+bw, shape[0]*(img_size+bw)+bw, 3), np.float32)\n-    for i in range(shape[1]):\n-        x_start = i*(img_size+bw)+bw\n-        for j in range(shape[0]):\n-            img_index = i*shape[0]+j\n-            if img_index>=nrof_images:\n-                break\n-            y_start = j*(img_size+bw)+bw\n-            img[x_start:x_start+img_size, y_start:y_start+img_size, :] = images[img_index, :, :, :]\n-        if img_index>=nrof_images:\n-            break\n-    return img\n-\n-def write_arguments_to_file(args, filename):\n-    with open(filename, \'w\') as f:\n-        for key, value in iteritems(vars(args)):\n-            f.write(\'%s: %s\\n\' % (key, str(value)))\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\ndeleted file mode 100644\nindex 3584c18..0000000\n--- a/src/freeze_graph.py\n+++ /dev/null\n@@ -1,103 +0,0 @@\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\n-and exports the model as a graphdef protobuf\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from tensorflow.python.framework import graph_util\n-import tensorflow as tf\n-import argparse\n-import os\n-import sys\n-import facenet\n-from six.moves import xrange  # @UnresolvedImport\n-\n-def main(args):\n-    with tf.Graph().as_default():\n-        with tf.Session() as sess:\n-            # Load the model metagraph and checkpoint\n-            print(\'Model directory: %s\' % args.model_dir)\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n-            \n-            print(\'Metagraph file: %s\' % meta_file)\n-            print(\'Checkpoint file: %s\' % ckpt_file)\n-\n-            model_dir_exp = os.path.expanduser(args.model_dir)\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n-            tf.get_default_session().run(tf.global_variables_initializer())\n-            tf.get_default_session().run(tf.local_variables_initializer())\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n-            \n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\n-            input_graph_def = sess.graph.as_graph_def()\n-            \n-            # Freeze the graph def\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\n-\n-        # Serialize and dump the output graph to the filesystem\n-        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n-            f.write(output_graph_def.SerializeToString())\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\n-        \n-def freeze_graph_def(sess, input_graph_def, output_node_names):\n-    for node in input_graph_def.node:\n-        if node.op == \'RefSwitch\':\n-            node.op = \'Switch\'\n-            for index in xrange(len(node.input)):\n-                if \'moving_\' in node.input[index]:\n-                    node.input[index] = node.input[index] + \'/read\'\n-        elif node.op == \'AssignSub\':\n-            node.op = \'Sub\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-        elif node.op == \'AssignAdd\':\n-            node.op = \'Add\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-    \n-    # Get the list of important nodes\n-    whitelist_names = []\n-    for node in input_graph_def.node:\n-        if (node.name.startswith(\'InceptionResnet\') or node.name.startswith(\'embeddings\') or \n-                node.name.startswith(\'image_batch\') or node.name.startswith(\'label_batch\') or\n-                node.name.startswith(\'phase_train\') or node.name.startswith(\'Logits\')):\n-            whitelist_names.append(node.name)\n-\n-    # Replace all the variables in the graph with constants of the same values\n-    output_graph_def = graph_util.convert_variables_to_constants(\n-        sess, input_graph_def, output_node_names.split(","),\n-        variable_names_whitelist=whitelist_names)\n-    return output_graph_def\n-  \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model_dir\', type=str, \n-        help=\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\')\n-    parser.add_argument(\'output_file\', type=str, \n-        help=\'Filename for the exported graphdef protobuf (.pb)\')\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/generative/a b/src/generative/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/src/generative/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/src/generative/calculate_attribute_vectors.py b/src/generative/calculate_attribute_vectors.py\ndeleted file mode 100644\nindex 8fe3ead..0000000\n--- a/src/generative/calculate_attribute_vectors.py\n+++ /dev/null\n@@ -1,200 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Calculate average latent variables (here called attribute vectors) \n-for the different attributes in CelebA\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import sys\n-import argparse\n-import importlib\n-import facenet\n-import os\n-import numpy as np\n-import math\n-import time\n-import h5py\n-from six import iteritems\n-\n-def main(args):\n-  \n-    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n-    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-    \n-    vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n-    \n-    fields, attribs_dict = read_annotations(args.annotations_filename)\n-    \n-    vae_def = importlib.import_module(args.vae_def)\n-    vae = vae_def.Vae(args.latent_var_size)\n-    gen_image_size = vae.get_image_size()\n-\n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        \n-        image_list = facenet.get_image_paths(os.path.expanduser(args.data_dir))\n-        \n-        # Get attributes for images\n-        nrof_attributes = len(fields)\n-        attribs_list = []\n-        for img in image_list:\n-            key = os.path.split(img)[1].split(\'.\')[0]\n-            attr = attribs_dict[key]\n-            assert len(attr)==nrof_attributes\n-            attribs_list.append(attr)\n-            \n-        # Create the input queue\n-        index_list = range(len(image_list))\n-        input_queue = tf.train.slice_input_producer([image_list, attribs_list, index_list], num_epochs=1, shuffle=False)        \n-        \n-        nrof_preprocess_threads = 4\n-        image_per_thread = []\n-        for _ in range(nrof_preprocess_threads):\n-            filename = input_queue[0]\n-            file_contents = tf.read_file(filename)\n-            image = tf.image.decode_image(file_contents, channels=3)\n-            image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n-            #image = tf.image.resize_images(image, (64,64))\n-            image.set_shape((args.image_size, args.image_size, 3))\n-            attrib = input_queue[1]\n-            attrib.set_shape((nrof_attributes,))\n-            image = tf.cast(image, tf.float32)\n-            image_per_thread.append([image, attrib, input_queue[2]])\n-    \n-        images, attribs, indices = tf.train.batch_join(\n-            image_per_thread, batch_size=args.batch_size, \n-            shapes=[(args.image_size, args.image_size, 3), (nrof_attributes,), ()], enqueue_many=False,\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        \n-        # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n-\n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n-        # Create encoder network\n-        mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n-        latent_var = mean + epsilon * std\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n-        # Start running operations on the Graph\n-        gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-        \n-\n-        with sess.as_default():\n-          \n-            if vae_checkpoint:\n-                print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n-                saver.restore(sess, vae_checkpoint)\n-           \n-            nrof_images = len(image_list)\n-            nrof_batches = int(math.ceil(len(image_list) / args.batch_size))\n-            latent_vars = np.zeros((nrof_images, args.latent_var_size))\n-            attributes = np.zeros((nrof_images, nrof_attributes))\n-            for i in range(nrof_batches):\n-                start_time = time.time()\n-                latent_var_, attribs_, indices_ = sess.run([latent_var, attribs, indices])\n-                latent_vars[indices_,:] = latent_var_\n-                attributes[indices_,:] = attribs_\n-                duration = time.time() - start_time\n-                print(\'Batch %d/%d: %.3f seconds\' % (i+1, nrof_batches, duration))\n-            # NOTE: This will print the \'Out of range\' warning if the last batch is not full,\n-            #  as described by https://github.com/tensorflow/tensorflow/issues/8330\n-             \n-            # Calculate average change in the latent variable when each attribute changes\n-            attribute_vectors = np.zeros((nrof_attributes, args.latent_var_size), np.float32)\n-            for i in range(nrof_attributes):\n-                pos_idx = np.argwhere(attributes[:,i]==1)[:,0]\n-                neg_idx = np.argwhere(attributes[:,i]==-1)[:,0]\n-                pos_avg = np.mean(latent_vars[pos_idx,:], 0)\n-                neg_avg = np.mean(latent_vars[neg_idx,:], 0)\n-                attribute_vectors[i,:] = pos_avg - neg_avg\n-            \n-            filename = os.path.expanduser(args.output_filename)\n-            print(\'Writing attribute vectors, latent variables and attributes to %s\' % filename)\n-            mdict = {\'latent_vars\':latent_vars, \'attributes\':attributes, \n-                     \'fields\':fields, \'attribute_vectors\':attribute_vectors }\n-            with h5py.File(filename, \'w\') as f:\n-                for key, value in iteritems(mdict):\n-                    f.create_dataset(key, data=value)\n-                    \n-                    \n-def read_annotations(filename):\n-    attribs = {}    \n-    with open(filename, \'r\') as f:\n-        for i, line in enumerate(f.readlines()):\n-            if i==0:\n-                continue  # First line is the number of entries in the file\n-            elif i==1:\n-                fields = line.strip().split() # Second line is the field names\n-            else:\n-                line = line.split()\n-                img_name = line[0].split(\'.\')[0]\n-                img_attribs = map(int, line[1:])\n-                attribs[img_name] = img_attribs\n-    return fields, attribs\n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\', \n-        default=\'src.generative.models.dfc_vae\')\n-    parser.add_argument(\'vae_checkpoint\', type=str,\n-        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the directory containing aligned face patches for the CelebA dataset.\')\n-    parser.add_argument(\'annotations_filename\', type=str,\n-        help=\'Path to the annotations file\',\n-        default=\'/media/deep/datasets/CelebA/Anno/list_attr_celeba.txt\')\n-    parser.add_argument(\'output_filename\', type=str,\n-        help=\'Filename to use for the file containing the attribute vectors.\')\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=128)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=64)\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-\n-    return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/generative/models/a b/src/generative/models/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/src/generative/models/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/src/generative/models/dfc_vae.py b/src/generative/models/dfc_vae.py\ndeleted file mode 100644\nindex b4450f2..0000000\n--- a/src/generative/models/dfc_vae.py\n+++ /dev/null\n@@ -1,92 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Variational autoencoder based on the paper \n-\'Deep Feature Consistent Variational Autoencoder\'\n-(https://arxiv.org/pdf/1610.00291.pdf)\n-"""\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n-\n-\n-class Vae(generative.models.vae_base.Vae):\n-  \n-    def __init__(self, latent_variable_dim):\n-        super(Vae, self).__init__(latent_variable_dim, 64)\n-  \n-    def encoder(self, images, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n-                    net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n-        return fc1, fc2\n-      \n-    def decoder(self, latent_var, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4\')\n-                \n-        return net\n-      \n-def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\ndiff --git a/src/generative/models/dfc_vae_large.py b/src/generative/models/dfc_vae_large.py\ndeleted file mode 100644\nindex aa8e8b7..0000000\n--- a/src/generative/models/dfc_vae_large.py\n+++ /dev/null\n@@ -1,95 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Variational autoencoder based on the paper \n-\'Deep Feature Consistent Variational Autoencoder\'\n-(https://arxiv.org/pdf/1610.00291.pdf) but with a larger image size (128x128 pixels)\n-"""\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n-\n-\n-class Vae(generative.models.vae_base.Vae):\n-  \n-    def __init__(self, latent_variable_dim):\n-        super(Vae, self).__init__(latent_variable_dim, 128)\n-        \n-      \n-    def encoder(self, images, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.conv2d(images, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4\')\n-                    net = slim.conv2d(net, 512, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_5\')\n-                    net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n-        return fc1, fc2\n-      \n-    def decoder(self, latent_var, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4\')\n-                \n-                    net = tf.image.resize_nearest_neighbor(net, size=(128,128), name=\'Upsample_5\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_5\')\n-        return net\n-\n-def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)  \ndiff --git a/src/generative/models/dfc_vae_resnet.py b/src/generative/models/dfc_vae_resnet.py\ndeleted file mode 100644\nindex 7c2f52c..0000000\n--- a/src/generative/models/dfc_vae_resnet.py\n+++ /dev/null\n@@ -1,110 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Variational autoencoder based on the paper \n-\'Deep Feature Consistent Variational Autoencoder\'\n-(https://arxiv.org/pdf/1610.00291.pdf)\n-"""\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import generative.models.vae_base  # @UnresolvedImport\n-\n-\n-class Vae(generative.models.vae_base.Vae):\n-  \n-    def __init__(self, latent_variable_dim):\n-        super(Vae, self).__init__(latent_variable_dim, 64)\n-  \n-    def encoder(self, images, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0\n-        with tf.variable_scope(\'encoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = images\n-                    \n-                    net = slim.conv2d(net, 32, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n-                    \n-                    net = slim.conv2d(net, 64, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n-\n-                    net = slim.conv2d(net, 128, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n-\n-                    net = slim.conv2d(net, 256, [4, 4], 2, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 256, [4, 4], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n-                    \n-                    net = slim.flatten(net)\n-                    fc1 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    fc2 = slim.fully_connected(net, self.latent_variable_dim, activation_fn=None, normalizer_fn=None, scope=\'Fc_2\')\n-        return fc1, fc2\n-      \n-    def decoder(self, latent_var, is_training):\n-        activation_fn = leaky_relu  # tf.nn.relu\n-        weight_decay = 0.0 \n-        with tf.variable_scope(\'decoder\'):\n-            with slim.arg_scope([slim.batch_norm],\n-                                is_training=is_training):\n-                with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                                    weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                                    weights_regularizer=slim.l2_regularizer(weight_decay),\n-                                    normalizer_fn=slim.batch_norm,\n-                                    normalizer_params=self.batch_norm_params):\n-                    net = slim.fully_connected(latent_var, 4096, activation_fn=None, normalizer_fn=None, scope=\'Fc_1\')\n-                    net = tf.reshape(net, [-1,4,4,256], name=\'Reshape\')\n-                    \n-                    net = tf.image.resize_nearest_neighbor(net, size=(8,8), name=\'Upsample_1\')\n-                    net = slim.conv2d(net, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 128, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_1b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(16,16), name=\'Upsample_2\')\n-                    net = slim.conv2d(net, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 64, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_2b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(32,32), name=\'Upsample_3\')\n-                    net = slim.conv2d(net, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 32, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_3b\')\n-            \n-                    net = tf.image.resize_nearest_neighbor(net, size=(64,64), name=\'Upsample_4\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4a\')\n-                    net = slim.repeat(net, 3, conv2d_block, 0.1, 3, [3, 3], 1, activation_fn=activation_fn, scope=\'Conv2d_4b\')\n-                    net = slim.conv2d(net, 3, [3, 3], 1, activation_fn=None, scope=\'Conv2d_4c\')\n-                \n-        return net\n-      \n-def conv2d_block(inp, scale, *args, **kwargs):\n-    return inp + slim.conv2d(inp, *args, **kwargs) * scale\n-\n-def leaky_relu(x):\n-    return tf.maximum(0.1*x,x)\n-  \n\\ No newline at end of file\ndiff --git a/src/generative/models/vae_base.py b/src/generative/models/vae_base.py\ndeleted file mode 100644\nindex 7437251..0000000\n--- a/src/generative/models/vae_base.py\n+++ /dev/null\n@@ -1,58 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Base class for variational autoencoders containing an encoder and a decoder\n-"""\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-\n-class Vae(object):\n-  \n-    def __init__(self, latent_variable_dim, image_size):\n-        self.latent_variable_dim = latent_variable_dim\n-        self.image_size = image_size\n-        self.batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-  \n-    def encoder(self, images, is_training):\n-        # Must be overridden in implementation classes\n-        raise NotImplementedError\n-      \n-    def decoder(self, latent_var, is_training):\n-        # Must be overridden in implementation classes\n-        raise NotImplementedError\n-\n-    def get_image_size(self):\n-        return self.image_size\n-        \n\\ No newline at end of file\ndiff --git a/src/generative/modify_attribute.py b/src/generative/modify_attribute.py\ndeleted file mode 100644\nindex 8187cff..0000000\n--- a/src/generative/modify_attribute.py\n+++ /dev/null\n@@ -1,142 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Modify attributes of images using attribute vectors calculated using\n-\'calculate_attribute_vectors.py\'. Images are generated from latent variables of\n-the CelebA dataset.\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import sys\n-import argparse\n-import importlib\n-import facenet\n-import os\n-import numpy as np\n-import h5py\n-import math\n-from scipy import misc\n-\n-def main(args):\n-  \n-    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n-    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-    \n-    vae_def = importlib.import_module(args.vae_def)\n-    vae = vae_def.Vae(args.latent_var_size)\n-    gen_image_size = vae.get_image_size()\n-\n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        \n-        images = tf.placeholder(tf.float32, shape=(None,gen_image_size,gen_image_size,3), name=\'input\')\n-        \n-        # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n-\n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n-        # Create encoder network\n-        mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n-        latent_var = mean + epsilon * std\n-        \n-        # Create decoder\n-        reconstructed_norm = vae.decoder(latent_var, False)\n-        \n-        # Un-normalize\n-        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n-\n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n-        # Start running operations on the Graph\n-        gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-        \n-\n-        with sess.as_default():\n-          \n-            vae_checkpoint = os.path.expanduser(args.vae_checkpoint)\n-            print(\'Restoring VAE checkpoint: %s\' % vae_checkpoint)\n-            saver.restore(sess, vae_checkpoint)\n-           \n-            filename = os.path.expanduser(args.attributes_filename)\n-            with h5py.File(filename,\'r\') as f:\n-                latent_vars = np.array(f.get(\'latent_vars\'))\n-                attributes = np.array(f.get(\'attributes\'))\n-                #fields = np.array(f.get(\'fields\'))\n-                attribute_vectors = np.array(f.get(\'attribute_vectors\'))\n-\n-            # Reconstruct faces while adding varying amount of the selected attribute vector\n-            attribute_index = 31 # 31: \'Smiling\'\n-            image_indices = [8,11,13,18,19,26,31,39,47,54,56,57,58,59,60,73]\n-            nrof_images = len(image_indices)\n-            nrof_interp_steps = 10\n-            sweep_latent_var = np.zeros((nrof_interp_steps*nrof_images, args.latent_var_size), np.float32)\n-            for j in range(nrof_images):\n-                image_index = image_indices[j]\n-                idx = np.argwhere(attributes[:,attribute_index]==-1)[image_index,0]\n-                for i in range(nrof_interp_steps):\n-                    sweep_latent_var[i+nrof_interp_steps*j,:] = latent_vars[idx,:] + 5.0*i/nrof_interp_steps*attribute_vectors[attribute_index,:]\n-                \n-            recon = sess.run(reconstructed, feed_dict={latent_var:sweep_latent_var})\n-            \n-            img = facenet.put_images_on_grid(recon, shape=(nrof_interp_steps*2,int(math.ceil(nrof_images/2))))\n-            \n-            image_filename = os.path.expanduser(args.output_image_filename)\n-            print(\'Writing generated image to %s\' % image_filename)\n-            misc.imsave(image_filename, img)\n-\n-                    \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n-    parser.add_argument(\'vae_checkpoint\', type=str,\n-        help=\'Checkpoint file of a pre-trained variational autoencoder.\')\n-    parser.add_argument(\'attributes_filename\', type=str,\n-        help=\'The file containing the attribute vectors, as generated by calculate_attribute_vectors.py.\')\n-    parser.add_argument(\'output_image_filename\', type=str,\n-        help=\'File to write the generated image to.\')\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-\n-    return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/generative/train_vae.py b/src/generative/train_vae.py\ndeleted file mode 100644\nindex c3c882f..0000000\n--- a/src/generative/train_vae.py\n+++ /dev/null\n@@ -1,284 +0,0 @@\n-# MIT License\n-# \n-# Copyright (c) 2017 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-"""Train a Variational Autoencoder\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import sys\n-import time\n-import importlib\n-import argparse\n-import facenet\n-import numpy as np\n-import h5py\n-import os\n-from datetime import datetime\n-from scipy import misc\n-from six import iteritems\n-\n-def main(args):\n-  \n-    img_mean = np.array([134.10714722, 102.52040863, 87.15436554])\n-    img_stddev = np.sqrt(np.array([3941.30175781, 2856.94287109, 2519.35791016]))\n-  \n-    vae_def = importlib.import_module(args.vae_def)\n-    vae = vae_def.Vae(args.latent_var_size)\n-    gen_image_size = vae.get_image_size()\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-    log_file_name = os.path.join(model_dir, \'logs.h5\')\n-    \n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(model_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, model_dir, \' \'.join(sys.argv))\n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-        \n-        train_set = facenet.get_dataset(args.data_dir)\n-        image_list, _ = facenet.get_image_paths_and_labels(train_set)\n-        \n-        # Create the input queue\n-        input_queue = tf.train.string_input_producer(image_list, shuffle=True)\n-    \n-        nrof_preprocess_threads = 4\n-        image_per_thread = []\n-        for _ in range(nrof_preprocess_threads):\n-            file_contents = tf.read_file(input_queue.dequeue())\n-            image = tf.image.decode_image(file_contents, channels=3)\n-            image = tf.image.resize_image_with_crop_or_pad(image, args.input_image_size, args.input_image_size)\n-            image.set_shape((args.input_image_size, args.input_image_size, 3))\n-            image = tf.cast(image, tf.float32)\n-            #pylint: disable=no-member\n-            image_per_thread.append([image])\n-    \n-        images = tf.train.batch_join(\n-            image_per_thread, batch_size=args.batch_size,\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=False)\n-        \n-        # Normalize\n-        images_norm = (images-img_mean) / img_stddev\n-\n-        # Resize to appropriate size for the encoder \n-        images_norm_resize = tf.image.resize_images(images_norm, (gen_image_size,gen_image_size))\n-        \n-        # Create encoder network\n-        mean, log_variance = vae.encoder(images_norm_resize, True)\n-        \n-        epsilon = tf.random_normal((tf.shape(mean)[0], args.latent_var_size))\n-        std = tf.exp(log_variance/2)\n-        latent_var = mean + epsilon * std\n-        \n-        # Create decoder network\n-        reconstructed_norm = vae.decoder(latent_var, True)\n-        \n-        # Un-normalize\n-        reconstructed = (reconstructed_norm*img_stddev) + img_mean\n-        \n-        # Create reconstruction loss\n-        if args.reconstruction_loss_type==\'PLAIN\':\n-            images_resize = tf.image.resize_images(images, (gen_image_size,gen_image_size))\n-            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(images_resize - reconstructed,2)))\n-        elif args.reconstruction_loss_type==\'PERCEPTUAL\':\n-            network = importlib.import_module(args.model_def)\n-\n-            reconstructed_norm_resize = tf.image.resize_images(reconstructed_norm, (args.input_image_size,args.input_image_size))\n-\n-            # Stack images from both the input batch and the reconstructed batch in a new tensor \n-            shp = [-1] + images_norm.get_shape().as_list()[1:]\n-            input_images = tf.reshape(tf.stack([images_norm, reconstructed_norm_resize], axis=0), shp)\n-            _, end_points = network.inference(input_images, 1.0, \n-                phase_train=False, bottleneck_layer_size=128, weight_decay=0.0)\n-\n-            # Get a list of feature names to use for loss terms\n-            feature_names = args.loss_features.replace(\' \', \'\').split(\',\')\n-\n-            # Calculate L2 loss between original and reconstructed images in feature space\n-            reconstruction_loss_list = []\n-            for feature_name in feature_names:\n-                feature_flat = slim.flatten(end_points[feature_name])\n-                image_feature, reconstructed_feature = tf.unstack(tf.reshape(feature_flat, [2,args.batch_size,-1]), num=2, axis=0)\n-                reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.pow(image_feature-reconstructed_feature, 2)), name=feature_name+\'_loss\')\n-                reconstruction_loss_list.append(reconstruction_loss)\n-            # Sum up the losses in for the different features\n-            reconstruction_loss = tf.add_n(reconstruction_loss_list, \'reconstruction_loss\')\n-        else:\n-            pass\n-        \n-        # Create KL divergence loss\n-        kl_loss = kl_divergence_loss(mean, log_variance)\n-        kl_loss_mean = tf.reduce_mean(kl_loss)\n-        \n-        total_loss = args.alfa*kl_loss_mean + args.beta*reconstruction_loss\n-        \n-        learning_rate = tf.train.exponential_decay(args.initial_learning_rate, global_step,\n-            args.learning_rate_decay_steps, args.learning_rate_decay_factor, staircase=True)\n-        \n-        # Calculate gradients and make sure not to include parameters for the perceptual loss model\n-        opt = tf.train.AdamOptimizer(learning_rate)\n-        grads = opt.compute_gradients(total_loss, var_list=get_variables_to_train())\n-        \n-        # Apply gradients\n-        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n-        with tf.control_dependencies([apply_gradient_op]):\n-            train_op = tf.no_op(name=\'train\')\n-\n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-        \n-        facenet_saver = tf.train.Saver(get_facenet_variables_to_restore())\n-\n-        # Start running operations on the Graph\n-        gpu_memory_fraction = 1.0\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-            \n-            if args.reconstruction_loss_type==\'PERCEPTUAL\':\n-                if not args.pretrained_model:\n-                    raise ValueError(\'A pretrained model must be specified when using perceptual loss\')\n-                pretrained_model_exp = os.path.expanduser(args.pretrained_model)\n-                print(\'Restoring pretrained model: %s\' % pretrained_model_exp)\n-                facenet_saver.restore(sess, pretrained_model_exp)\n-          \n-            log = {\n-                \'total_loss\': np.zeros((0,), np.float),\n-                \'reconstruction_loss\': np.zeros((0,), np.float),\n-                \'kl_loss\': np.zeros((0,), np.float),\n-                \'learning_rate\': np.zeros((0,), np.float),\n-                }\n-            \n-            step = 0\n-            print(\'Running training\')\n-            while step < args.max_nrof_steps:\n-                start_time = time.time()\n-                step += 1\n-                save_state = step>0 and (step % args.save_every_n_steps==0 or step==args.max_nrof_steps)\n-                if save_state:\n-                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_, rec_ = sess.run(\n-                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate, reconstructed])\n-                    img = facenet.put_images_on_grid(rec_, shape=(16,8))\n-                    misc.imsave(os.path.join(model_dir, \'reconstructed_%06d.png\' % step), img)\n-                else:\n-                    _, reconstruction_loss_, kl_loss_mean_, total_loss_, learning_rate_ = sess.run(\n-                          [train_op, reconstruction_loss, kl_loss_mean, total_loss, learning_rate])\n-                log[\'total_loss\'] = np.append(log[\'total_loss\'], total_loss_)\n-                log[\'reconstruction_loss\'] = np.append(log[\'reconstruction_loss\'], reconstruction_loss_)\n-                log[\'kl_loss\'] = np.append(log[\'kl_loss\'], kl_loss_mean_)\n-                log[\'learning_rate\'] = np.append(log[\'learning_rate\'], learning_rate_)\n-\n-                duration = time.time() - start_time\n-                print(\'Step: %d \\tTime: %.3f \\trec_loss: %.3f \\tkl_loss: %.3f \\ttotal_loss: %.3f\' % (step, duration, reconstruction_loss_, kl_loss_mean_, total_loss_))\n-\n-                if save_state:\n-                    print(\'Saving checkpoint file\')\n-                    checkpoint_path = os.path.join(model_dir, \'model.ckpt\')\n-                    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-                    print(\'Saving log\')\n-                    with h5py.File(log_file_name, \'w\') as f:\n-                        for key, value in iteritems(log):\n-                            f.create_dataset(key, data=value)\n-\n-def get_variables_to_train():\n-    train_variables = []\n-    for var in tf.trainable_variables():\n-        if \'Inception\' not in var.name:\n-            train_variables.append(var)\n-    return train_variables\n-\n-def get_facenet_variables_to_restore():\n-    facenet_variables = []\n-    for var in tf.global_variables():\n-        if var.name.startswith(\'Inception\'):\n-            if \'Adam\' not in var.name:\n-                facenet_variables.append(var)\n-    return facenet_variables\n-\n-def kl_divergence_loss(mean, log_variance):\n-    kl = 0.5 * tf.reduce_sum( tf.exp(log_variance) + tf.square(mean) - 1.0 - log_variance, reduction_indices = 1)\n-    return kl\n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'vae_def\', type=str,\n-        help=\'Model definition for the variational autoencoder. Points to a module containing the definition.\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\')\n-    parser.add_argument(\'model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\')\n-    parser.add_argument(\'pretrained_model\', type=str,\n-        help=\'Pretrained model to use to calculate features for perceptual loss.\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/vae\')\n-    parser.add_argument(\'--loss_features\', type=str,\n-        help=\'Comma separated list of features to use for perceptual loss. Features should be defined \' +\n-          \'in the end_points dictionary.\', default=\'Conv2d_1a_3x3,Conv2d_2a_3x3, Conv2d_2b_3x3\')\n-    parser.add_argument(\'--reconstruction_loss_type\', type=str, choices=[\'PLAIN\', \'PERCEPTUAL\'],\n-        help=\'The type of reconstruction loss to use\', default=\'PERCEPTUAL\')\n-    parser.add_argument(\'--max_nrof_steps\', type=int,\n-        help=\'Number of steps to run.\', default=50000)\n-    parser.add_argument(\'--save_every_n_steps\', type=int,\n-        help=\'Number of steps between storing of model checkpoint and log files\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=128)\n-    parser.add_argument(\'--input_image_size\', type=int,\n-        help=\'Image size of input images (height, width) in pixels. If perceptual loss is used this \' \n-        + \'should be the input image size for the perceptual loss model\', default=160)\n-    parser.add_argument(\'--latent_var_size\', type=int,\n-        help=\'Dimensionality of the latent variable.\', default=100)\n-    parser.add_argument(\'--initial_learning_rate\', type=float,\n-        help=\'Initial learning rate.\', default=0.0005)\n-    parser.add_argument(\'--learning_rate_decay_steps\', type=int,\n-        help=\'Number of steps between learning rate decay.\', default=1)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--alfa\', type=float,\n-        help=\'Kullback-Leibler divergence loss factor.\', default=1.0)\n-    parser.add_argument(\'--beta\', type=float,\n-        help=\'Reconstruction loss factor.\', default=0.5)\n-    \n-    return parser.parse_args(argv)\n-  \n-    \n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/lfw.py b/src/lfw.py\ndeleted file mode 100644\nindex 9194433..0000000\n--- a/src/lfw.py\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \n-"""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import os\n-import numpy as np\n-import facenet\n-\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    # Calculate evaluation metrics\n-    thresholds = np.arange(0, 4, 0.01)\n-    embeddings1 = embeddings[0::2]\n-    embeddings2 = embeddings[1::2]\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    thresholds = np.arange(0, 4, 0.001)\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    return tpr, fpr, accuracy, val, val_std, far\n-\n-def get_paths(lfw_dir, pairs):\n-    nrof_skipped_pairs = 0\n-    path_list = []\n-    issame_list = []\n-    for pair in pairs:\n-        if len(pair) == 3:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2])))\n-            issame = True\n-        elif len(pair) == 4:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3])))\n-            issame = False\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n-            path_list += (path0,path1)\n-            issame_list.append(issame)\n-        else:\n-            nrof_skipped_pairs += 1\n-    if nrof_skipped_pairs>0:\n-        print(\'Skipped %d image pairs\' % nrof_skipped_pairs)\n-    \n-    return path_list, issame_list\n-  \n-def add_extension(path):\n-    if os.path.exists(path+\'.jpg\'):\n-        return path+\'.jpg\'\n-    elif os.path.exists(path+\'.png\'):\n-        return path+\'.png\'\n-    else:\n-        raise RuntimeError(\'No file "%s" with extension png or jpg.\' % path)\n-\n-def read_pairs(pairs_filename):\n-    pairs = []\n-    with open(pairs_filename, \'r\') as f:\n-        for line in f.readlines()[1:]:\n-            pair = line.strip().split()\n-            pairs.append(pair)\n-    return np.array(pairs)\n-\n-\n-\ndiff --git a/src/models/__init__.py b/src/models/__init__.py\ndeleted file mode 100644\nindex efa6252..0000000\n--- a/src/models/__init__.py\n+++ /dev/null\n@@ -1,2 +0,0 @@\n-# flake8: noqa\n-\ndiff --git a/src/models/a b/src/models/a\ndeleted file mode 100644\nindex 8b13789..0000000\n--- a/src/models/a\n+++ /dev/null\n@@ -1 +0,0 @@\n-\ndiff --git a/src/models/dummy.py b/src/models/dummy.py\ndeleted file mode 100644\nindex 7afe1ef..0000000\n--- a/src/models/dummy.py\n+++ /dev/null\n@@ -1,54 +0,0 @@\n-"""Dummy model used only for testing\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-import numpy as np\n-  \n-def inference(images, keep_probability, phase_train=True,  # @UnusedVariable\n-              bottleneck_layer_size=128, bottleneck_layer_activation=None, weight_decay=0.0, reuse=None):  # @UnusedVariable\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        size = np.prod(images.get_shape()[1:].as_list())\n-        net = slim.fully_connected(tf.reshape(images, (-1,size)), bottleneck_layer_size, activation_fn=None, \n-                scope=\'Bottleneck\', reuse=False)\n-        return net, None\ndiff --git a/src/models/inception_resnet_v1.py b/src/models/inception_resnet_v1.py\ndeleted file mode 100644\nindex 475e81b..0000000\n--- a/src/models/inception_resnet_v1.py\n+++ /dev/null\n@@ -1,246 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-"""Contains the definition of the Inception Resnet V1 architecture.\n-As described in http://arxiv.org/abs/1602.07261.\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n-    on Learning\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-# Inception-Resnet-A\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 32, 3, scope=\'Conv2d_0c_3x3\')\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-# Inception-Resnet-B\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 128, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 128, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 128, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-\n-# Inception-Resnet-C\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 192, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-  \n-def reduction_a(net, k, l, m, n):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, n, 3, stride=2, padding=\'VALID\',\n-                                 scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1_0 = slim.conv2d(net, k, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1_0, l, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv1_2 = slim.conv2d(tower_conv1_1, m, 3,\n-                                    stride=2, padding=\'VALID\',\n-                                    scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n-    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n-    return net\n-\n-def reduction_b(net):\n-    with tf.variable_scope(\'Branch_0\'):\n-        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_1\'):\n-        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv1_1 = slim.conv2d(tower_conv1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_2\'):\n-        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-        tower_conv2_1 = slim.conv2d(tower_conv2, 256, 3,\n-                                    scope=\'Conv2d_0b_3x3\')\n-        tower_conv2_2 = slim.conv2d(tower_conv2_1, 256, 3, stride=2,\n-                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-    with tf.variable_scope(\'Branch_3\'):\n-        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                     scope=\'MaxPool_1a_3x3\')\n-    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                        tower_conv2_2, tower_pool], 3)\n-    return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    \n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v1(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v1(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None, \n-                        scope=\'InceptionResnetV1\'):\n-    """Creates the Inception Resnet V1 model.\n-    Args:\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n-      num_classes: number of predicted classes.\n-      is_training: whether is training or not.\n-      dropout_keep_prob: float, the fraction to keep before final layer.\n-      reuse: whether or not the network and its variables should be reused. To be\n-        able to reuse \'scope\' must be given.\n-      scope: Optional variable_scope.\n-    Returns:\n-      logits: the logits outputs of the model.\n-      end_points: the set of end_points from the inception model.\n-    """\n-    end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV1\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n-                # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n-                # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n-                # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n-                # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n-                # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n-                # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n-                # 35 x 35 x 256\n-                net = slim.conv2d(net, 256, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_4b_3x3\')\n-                end_points[\'Conv2d_4b_3x3\'] = net\n-                \n-                # 5 x Inception-resnet-A\n-                net = slim.repeat(net, 5, block35, scale=0.17)\n-                end_points[\'Mixed_5a\'] = net\n-        \n-                # Reduction-A\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    net = reduction_a(net, 192, 192, 256, 384)\n-                end_points[\'Mixed_6a\'] = net\n-                \n-                # 10 x Inception-Resnet-B\n-                net = slim.repeat(net, 10, block17, scale=0.10)\n-                end_points[\'Mixed_6b\'] = net\n-                \n-                # Reduction-B\n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    net = reduction_b(net)\n-                end_points[\'Mixed_7a\'] = net\n-                \n-                # 5 x Inception-Resnet-C\n-                net = slim.repeat(net, 5, block8, scale=0.20)\n-                end_points[\'Mixed_8a\'] = net\n-                \n-                net = block8(net, activation_fn=None)\n-                end_points[\'Mixed_8b\'] = net\n-                \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n-                    net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n-    return net, end_points\ndiff --git a/src/models/inception_resnet_v2.py b/src/models/inception_resnet_v2.py\ndeleted file mode 100644\nindex 0fb176f..0000000\n--- a/src/models/inception_resnet_v2.py\n+++ /dev/null\n@@ -1,255 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the "License");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-# http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an "AS IS" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-"""Contains the definition of the Inception Resnet V2 architecture.\n-As described in http://arxiv.org/abs/1602.07261.\n-  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n-    on Learning\n-  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n-"""\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-# Inception-Resnet-A\n-def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 35x35 resnet block."""\n-    with tf.variable_scope(scope, \'Block35\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 32, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope=\'Conv2d_0b_3x3\')\n-        with tf.variable_scope(\'Branch_2\'):\n-            tower_conv2_0 = slim.conv2d(net, 32, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope=\'Conv2d_0b_3x3\')\n-            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope=\'Conv2d_0c_3x3\')\n-        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-# Inception-Resnet-B\n-def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 17x17 resnet block."""\n-    with tf.variable_scope(scope, \'Block17\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 128, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n-                                        scope=\'Conv2d_0b_1x7\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n-                                        scope=\'Conv2d_0c_7x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-\n-\n-# Inception-Resnet-C\n-def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n-    """Builds the 8x8 resnet block."""\n-    with tf.variable_scope(scope, \'Block8\', [net], reuse=reuse):\n-        with tf.variable_scope(\'Branch_0\'):\n-            tower_conv = slim.conv2d(net, 192, 1, scope=\'Conv2d_1x1\')\n-        with tf.variable_scope(\'Branch_1\'):\n-            tower_conv1_0 = slim.conv2d(net, 192, 1, scope=\'Conv2d_0a_1x1\')\n-            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n-                                        scope=\'Conv2d_0b_1x3\')\n-            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n-                                        scope=\'Conv2d_0c_3x1\')\n-        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n-        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n-                         activation_fn=None, scope=\'Conv2d_1x1\')\n-        net += scale * up\n-        if activation_fn:\n-            net = activation_fn(net)\n-    return net\n-  \n-def inference(images, keep_probability, phase_train=True, \n-              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-}\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.initializers.xavier_initializer(), \n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        return inception_resnet_v2(images, is_training=phase_train,\n-              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n-\n-\n-def inception_resnet_v2(inputs, is_training=True,\n-                        dropout_keep_prob=0.8,\n-                        bottleneck_layer_size=128,\n-                        reuse=None,\n-                        scope=\'InceptionResnetV2\'):\n-    """Creates the Inception Resnet V2 model.\n-    Args:\n-      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n-      num_classes: number of predicted classes.\n-      is_training: whether is training or not.\n-      dropout_keep_prob: float, the fraction to keep before final layer.\n-      reuse: whether or not the network and its variables should be reused. To be\n-        able to reuse \'scope\' must be given.\n-      scope: Optional variable_scope.\n-    Returns:\n-      logits: the logits outputs of the model.\n-      end_points: the set of end_points from the inception model.\n-    """\n-    end_points = {}\n-  \n-    with tf.variable_scope(scope, \'InceptionResnetV2\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                            is_training=is_training):\n-            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n-                                stride=1, padding=\'SAME\'):\n-      \n-                # 149 x 149 x 32\n-                net = slim.conv2d(inputs, 32, 3, stride=2, padding=\'VALID\',\n-                                  scope=\'Conv2d_1a_3x3\')\n-                end_points[\'Conv2d_1a_3x3\'] = net\n-                # 147 x 147 x 32\n-                net = slim.conv2d(net, 32, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_2a_3x3\')\n-                end_points[\'Conv2d_2a_3x3\'] = net\n-                # 147 x 147 x 64\n-                net = slim.conv2d(net, 64, 3, scope=\'Conv2d_2b_3x3\')\n-                end_points[\'Conv2d_2b_3x3\'] = net\n-                # 73 x 73 x 64\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_3a_3x3\')\n-                end_points[\'MaxPool_3a_3x3\'] = net\n-                # 73 x 73 x 80\n-                net = slim.conv2d(net, 80, 1, padding=\'VALID\',\n-                                  scope=\'Conv2d_3b_1x1\')\n-                end_points[\'Conv2d_3b_1x1\'] = net\n-                # 71 x 71 x 192\n-                net = slim.conv2d(net, 192, 3, padding=\'VALID\',\n-                                  scope=\'Conv2d_4a_3x3\')\n-                end_points[\'Conv2d_4a_3x3\'] = net\n-                # 35 x 35 x 192\n-                net = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                      scope=\'MaxPool_5a_3x3\')\n-                end_points[\'MaxPool_5a_3x3\'] = net\n-        \n-                # 35 x 35 x 320\n-                with tf.variable_scope(\'Mixed_5b\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 96, 1, scope=\'Conv2d_1x1\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n-                                                    scope=\'Conv2d_0b_5x5\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n-                                                    scope=\'Conv2d_0c_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding=\'SAME\',\n-                                                     scope=\'AvgPool_0a_3x3\')\n-                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n-                                                   scope=\'Conv2d_0b_1x1\')\n-                    net = tf.concat([tower_conv, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool_1], 3)\n-        \n-                end_points[\'Mixed_5b\'] = net\n-                net = slim.repeat(net, 10, block35, scale=0.17)\n-        \n-                # 17 x 17 x 1024\n-                with tf.variable_scope(\'Mixed_6a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding=\'VALID\',\n-                                                 scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n-                                                    stride=2, padding=\'VALID\',\n-                                                    scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n-                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_6a\'] = net\n-                net = slim.repeat(net, 20, block17, scale=0.10)\n-        \n-                with tf.variable_scope(\'Mixed_7a\'):\n-                    with tf.variable_scope(\'Branch_0\'):\n-                        tower_conv = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n-                                                   padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_1\'):\n-                        tower_conv1 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_2\'):\n-                        tower_conv2 = slim.conv2d(net, 256, 1, scope=\'Conv2d_0a_1x1\')\n-                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n-                                                    scope=\'Conv2d_0b_3x3\')\n-                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n-                                                    padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n-                    with tf.variable_scope(\'Branch_3\'):\n-                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding=\'VALID\',\n-                                                     scope=\'MaxPool_1a_3x3\')\n-                    net = tf.concat([tower_conv_1, tower_conv1_1,\n-                                        tower_conv2_2, tower_pool], 3)\n-        \n-                end_points[\'Mixed_7a\'] = net\n-        \n-                net = slim.repeat(net, 9, block8, scale=0.20)\n-                net = block8(net, activation_fn=None)\n-        \n-                net = slim.conv2d(net, 1536, 1, scope=\'Conv2d_7b_1x1\')\n-                end_points[\'Conv2d_7b_1x1\'] = net\n-        \n-                with tf.variable_scope(\'Logits\'):\n-                    end_points[\'PrePool\'] = net\n-                    #pylint: disable=no-member\n-                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding=\'VALID\',\n-                                          scope=\'AvgPool_1a_8x8\')\n-                    net = slim.flatten(net)\n-          \n-                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n-                                       scope=\'Dropout\')\n-          \n-                    end_points[\'PreLogitsFlatten\'] = net\n-                \n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-  \n-    return net, end_points\ndiff --git a/src/models/squeezenet.py b/src/models/squeezenet.py\ndeleted file mode 100644\nindex ae117e1..0000000\n--- a/src/models/squeezenet.py\n+++ /dev/null\n@@ -1,67 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import tensorflow.contrib.slim as slim\n-\n-def fire_module(inputs,\n-                squeeze_depth,\n-                expand_depth,\n-                reuse=None,\n-                scope=None,\n-                outputs_collections=None):\n-    with tf.variable_scope(scope, \'fire\', [inputs], reuse=reuse):\n-        with slim.arg_scope([slim.conv2d, slim.max_pool2d],\n-                            outputs_collections=None):\n-            net = squeeze(inputs, squeeze_depth)\n-            outputs = expand(net, expand_depth)\n-            return outputs\n-\n-def squeeze(inputs, num_outputs):\n-    return slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'squeeze\')\n-\n-def expand(inputs, num_outputs):\n-    with tf.variable_scope(\'expand\'):\n-        e1x1 = slim.conv2d(inputs, num_outputs, [1, 1], stride=1, scope=\'1x1\')\n-        e3x3 = slim.conv2d(inputs, num_outputs, [3, 3], scope=\'3x3\')\n-    return tf.concat([e1x1, e3x3], 3)\n-\n-def inference(images, keep_probability, phase_train=True, bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n-    batch_norm_params = {\n-        # Decay for the moving averages.\n-        \'decay\': 0.995,\n-        # epsilon to prevent 0s in variance.\n-        \'epsilon\': 0.001,\n-        # force in-place updates of mean and variance estimates\n-        \'updates_collections\': None,\n-        # Moving averages ends up in the trainable variables collection\n-        \'variables_collections\': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n-    }\n-    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n-                        weights_initializer=slim.xavier_initializer_conv2d(uniform=True),\n-                        weights_regularizer=slim.l2_regularizer(weight_decay),\n-                        normalizer_fn=slim.batch_norm,\n-                        normalizer_params=batch_norm_params):\n-        with tf.variable_scope(\'squeezenet\', [images], reuse=reuse):\n-            with slim.arg_scope([slim.batch_norm, slim.dropout],\n-                                is_training=phase_train):\n-                net = slim.conv2d(images, 96, [7, 7], stride=2, scope=\'conv1\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool1\')\n-                net = fire_module(net, 16, 64, scope=\'fire2\')\n-                net = fire_module(net, 16, 64, scope=\'fire3\')\n-                net = fire_module(net, 32, 128, scope=\'fire4\')\n-                net = slim.max_pool2d(net, [2, 2], stride=2, scope=\'maxpool4\')\n-                net = fire_module(net, 32, 128, scope=\'fire5\')\n-                net = fire_module(net, 48, 192, scope=\'fire6\')\n-                net = fire_module(net, 48, 192, scope=\'fire7\')\n-                net = fire_module(net, 64, 256, scope=\'fire8\')\n-                net = slim.max_pool2d(net, [3, 3], stride=2, scope=\'maxpool8\')\n-                net = fire_module(net, 64, 256, scope=\'fire9\')\n-                net = slim.dropout(net, keep_probability)\n-                net = slim.conv2d(net, 1000, [1, 1], activation_fn=None, normalizer_fn=None, scope=\'conv10\')\n-                net = slim.avg_pool2d(net, net.get_shape()[1:3], scope=\'avgpool10\')\n-                net = tf.squeeze(net, [1, 2], name=\'logits\')\n-                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n-                        scope=\'Bottleneck\', reuse=False)\n-    return net, None\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\ndeleted file mode 100644\nindex 6b0b28b..0000000\n--- a/src/train_softmax.py\n+++ /dev/null\n@@ -1,580 +0,0 @@\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from datetime import datetime\n-import os.path\n-import time\n-import sys\n-import random\n-import tensorflow as tf\n-import numpy as np\n-import importlib\n-import argparse\n-import facenet\n-import lfw\n-import h5py\n-import math\n-import tensorflow.contrib.slim as slim\n-from tensorflow.python.ops import data_flow_ops\n-from tensorflow.python.framework import ops\n-from tensorflow.python.ops import array_ops\n-\n-def main(args):\n-  \n-    network = importlib.import_module(args.model_def)\n-    image_size = (args.image_size, args.image_size)\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n-        os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-\n-    stat_file_name = os.path.join(log_dir, \'stat.h5\')\n-\n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n-\n-    np.random.seed(seed=args.seed)\n-    random.seed(args.seed)\n-    dataset = facenet.get_dataset(args.data_dir)\n-    if args.filter_filename:\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\n-        \n-    if args.validation_set_split_ratio>0.0:\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \'SPLIT_IMAGES\')\n-    else:\n-        train_set, val_set = dataset, []\n-        \n-    nrof_classes = len(train_set)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n-    pretrained_model = None\n-    if args.pretrained_model:\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\n-        print(\'Pre-trained model: %s\' % pretrained_model)\n-    \n-    if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n-        # Read the file containing the pairs used for testing\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-        # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-        \n-        # Get a list of image paths and their labels\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n-        assert len(image_list)>0, \'The training set should not be empty\'\n-        \n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\n-\n-        # Create a queue that produces indices into the image_list and label_list \n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n-        range_size = array_ops.shape(labels)[0]\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\n-                             shuffle=True, seed=None, capacity=32)\n-        \n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \'index_dequeue\')\n-        \n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-        \n-        nrof_preprocess_threads = 4\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\n-                                    shapes=[(1,), (1,), (1,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'enqueue_op\')\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        label_batch = tf.identity(label_batch, \'label_batch\')\n-        \n-        print(\'Number of classes in training set: %d\' % nrof_classes)\n-        print(\'Number of examples in training set: %d\' % len(image_list))\n-\n-        print(\'Number of classes in validation set: %d\' % len(val_set))\n-        print(\'Number of examples in validation set: %d\' % len(val_image_list))\n-        \n-        print(\'Building training graph\')\n-        \n-        # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \n-            weight_decay=args.weight_decay)\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \n-                weights_initializer=slim.initializers.xavier_initializer(), \n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\n-                scope=\'Logits\', reuse=False)\n-\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n-\n-        # Norm for the prelogits\n-        eps = 1e-4\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n-\n-        # Add center loss\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n-\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n-\n-        # Calculate the average cross entropy loss across the batch\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n-            labels=label_batch, logits=logits, name=\'cross_entropy_per_example\')\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n-        tf.add_to_collection(\'losses\', cross_entropy_mean)\n-        \n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\n-        accuracy = tf.reduce_mean(correct_prediction)\n-        \n-        # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n-\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-\n-        # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n-\n-        # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-\n-            if pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % pretrained_model)\n-                saver.restore(sess, pretrained_model)\n-\n-            # Training and validation loop\n-            print(\'Running training\')\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\n-            stat = {\n-                \'loss\': np.zeros((nrof_steps,), np.float32),\n-                \'center_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'reg_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'xent_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'prelogits_norm\': np.zeros((nrof_steps,), np.float32),\n-                \'accuracy\': np.zeros((nrof_steps,), np.float32),\n-                \'val_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_xent_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_accuracy\': np.zeros((nrof_val_samples,), np.float32),\n-                \'lfw_accuracy\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'lfw_valrate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'learning_rate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_train\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_validate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_evaluate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'prelogits_hist\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\n-              }\n-            for epoch in range(1,args.max_nrof_epochs+1):\n-                step = sess.run(global_step, feed_dict=None)\n-                # Train for one epoch\n-                t = time.time()\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\n-                stat[\'time_train\'][epoch-1] = time.time() - t\n-                \n-                if not cont:\n-                    break\n-                  \n-                t = time.time()\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-                        phase_train_placeholder, batch_size_placeholder, \n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\n-                stat[\'time_validate\'][epoch-1] = time.time() - t\n-\n-                # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\n-\n-                # Evaluate on LFW\n-                t = time.time()\n-                if args.lfw_dir:\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\n-                stat[\'time_evaluate\'][epoch-1] = time.time() - t\n-\n-                print(\'Saving statistics\')\n-                with h5py.File(stat_file_name, \'w\') as f:\n-                    for key, value in stat.iteritems():\n-                        f.create_dataset(key, data=value)\n-    \n-    return model_dir\n-  \n-def find_threshold(var, percentile):\n-    hist, bin_edges = np.histogram(var, 100)\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n-    #plt.plot(bin_centers, cdf)\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\n-    return threshold\n-  \n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\n-    with h5py.File(data_filename,\'r\') as f:\n-        distance_to_center = np.array(f.get(\'distance_to_center\'))\n-        label_list = np.array(f.get(\'label_list\'))\n-        image_list = np.array(f.get(\'image_list\'))\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\n-        filtered_dataset = dataset\n-        removelist = []\n-        for i in indices:\n-            label = label_list[i]\n-            image = image_list[i]\n-            if image in filtered_dataset[label].image_paths:\n-                filtered_dataset[label].image_paths.remove(image)\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\n-                removelist.append(label)\n-\n-        ix = sorted(list(set(removelist)), reverse=True)\n-        for i in ix:\n-            del(filtered_dataset[i])\n-\n-    return filtered_dataset\n-  \n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \n-      stat, cross_entropy_mean, accuracy, \n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\n-    batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n-        lr = args.learning_rate\n-    else:\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-        \n-    if lr<=0:\n-        return False \n-\n-    index_epoch = sess.run(index_dequeue_op)\n-    label_epoch = np.array(label_list)[index_epoch]\n-    image_epoch = np.array(image_list)[index_epoch]\n-    \n-    # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n-    control_array = np.ones_like(labels_array) * control_value\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-\n-    # Training loop\n-    train_time = 0\n-    while batch_number < args.epoch_size:\n-        start_time = time.time()\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\n-        if batch_number % 100 == 0:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\n-            summary_writer.add_summary(summary_str, global_step=step_)\n-        else:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\n-         \n-        duration = time.time() - start_time\n-        stat[\'loss\'][step_-1] = loss_\n-        stat[\'center_loss\'][step_-1] = center_loss_\n-        stat[\'reg_loss\'][step_-1] = np.sum(reg_losses_)\n-        stat[\'xent_loss\'][step_-1] = cross_entropy_mean_\n-        stat[\'prelogits_norm\'][step_-1] = prelogits_norm_\n-        stat[\'learning_rate\'][epoch-1] = lr_\n-        stat[\'accuracy\'][step_-1] = accuracy_\n-        stat[\'prelogits_hist\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\n-        \n-        duration = time.time() - start_time\n-        print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tRegLoss %2.3f\\tAccuracy %2.3f\\tLr %2.5f\\tCl %2.3f\' %\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\n-        batch_number += 1\n-        train_time += duration\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/total\', simple_value=train_time)\n-    summary_writer.add_summary(summary, global_step=step_)\n-    return True\n-\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-             phase_train_placeholder, batch_size_placeholder, \n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\n-  \n-    print(\'Running forward pass on validation set\')\n-\n-    nrof_batches = len(label_list) // args.lfw_batch_size\n-    nrof_images = nrof_batches * args.lfw_batch_size\n-    \n-    # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-\n-    loss_array = np.zeros((nrof_batches,), np.float32)\n-    xent_array = np.zeros((nrof_batches,), np.float32)\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\n-\n-    # Training loop\n-    start_time = time.time()\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-\n-    duration = time.time() - start_time\n-\n-    val_index = (epoch-1)//validate_every_n_epochs\n-    stat[\'val_loss\'][val_index] = np.mean(loss_array)\n-    stat[\'val_xent_loss\'][val_index] = np.mean(xent_array)\n-    stat[\'val_accuracy\'][val_index] = np.mean(accuracy_array)\n-\n-    print(\'Validation Epoch: %d\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tAccuracy %2.3f\' %\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\n-\n-\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n-    start_time = time.time()\n-    # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n-    # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n-    nrof_flips = 2 if use_flipped_images else 1\n-    nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n-    control_array = np.zeros_like(labels_array, np.int32)\n-    if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n-    if use_flipped_images:\n-        # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n-    embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n-    nrof_batches = nrof_images // batch_size\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    lab_array = np.zeros((nrof_images,))\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n-        lab_array[lab] = lab\n-        emb_array[lab, :] = emb\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n-    if use_flipped_images:\n-        # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n-    else:\n-        embeddings = emb_array\n-\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    lfw_time = time.time() - start_time\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n-    summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n-    stat[\'lfw_accuracy\'][epoch-1] = np.mean(accuracy)\n-    stat[\'lfw_valrate\'][epoch-1] = val\n-\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n-    # Save the model checkpoint\n-    print(\'Saving variables\')\n-    start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-    save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n-    if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n-        start_time = time.time()\n-        saver.export_meta_graph(metagraph_filename)\n-        save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n-    summary_writer.add_summary(summary, step)\n-  \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--random_rotate\', \n-        help=\'Performs random rotations of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--center_loss_factor\', type=float,\n-        help=\'Center loss factor.\', default=0.0)\n-    parser.add_argument(\'--center_loss_alfa\', type=float,\n-        help=\'Center update rate for center loss.\', default=0.95)\n-    parser.add_argument(\'--prelogits_norm_loss_factor\', type=float,\n-        help=\'Loss based on the norm of the activations in the prelogits layer.\', default=0.0)\n-    parser.add_argument(\'--prelogits_norm_p\', type=float,\n-        help=\'Norm to use for prelogits norm loss.\', default=1.0)\n-    parser.add_argument(\'--prelogits_hist_max\', type=float,\n-        help=\'The max value for the prelogits histogram.\', default=10.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--nrof_preprocess_threads\', type=int,\n-        help=\'Number of preprocessing (data loading and augmentation) threads.\', default=4)\n-    parser.add_argument(\'--log_histograms\', \n-        help=\'Enables logging of weight/bias histograms in tensorboard.\', action=\'store_true\')\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-    parser.add_argument(\'--filter_filename\', type=str,\n-        help=\'File containing image data used for dataset filtering\', default=\'\')\n-    parser.add_argument(\'--filter_percentile\', type=float,\n-        help=\'Keep only the percentile images closed to its class center\', default=100.0)\n-    parser.add_argument(\'--filter_min_nrof_images_per_class\', type=int,\n-        help=\'Keep only the classes with this number of examples or more\', default=0)\n-    parser.add_argument(\'--validate_every_n_epochs\', type=int,\n-        help=\'Number of epoch between validation\', default=5)\n-    parser.add_argument(\'--validation_set_split_ratio\', type=float,\n-        help=\'The ratio of the total dataset to use for validation\', default=0.0)\n-    parser.add_argument(\'--min_nrof_val_images_per_class\', type=float,\n-        help=\'Classes with fewer images will be removed from the validation set\', default=0)\n- \n-    # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--lfw_distance_metric\', type=int,\n-        help=\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\', default=0)\n-    parser.add_argument(\'--lfw_use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--lfw_subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n-    return parser.parse_args(argv)\n-  \n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\ndeleted file mode 100644\nindex d6df19a..0000000\n--- a/src/train_tripletloss.py\n+++ /dev/null\n@@ -1,486 +0,0 @@\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from datetime import datetime\n-import os.path\n-import time\n-import sys\n-import tensorflow as tf\n-import numpy as np\n-import importlib\n-import itertools\n-import argparse\n-import facenet\n-import lfw\n-\n-from tensorflow.python.ops import data_flow_ops\n-\n-from six.moves import xrange  # @UnresolvedImport\n-\n-def main(args):\n-  \n-    network = importlib.import_module(args.model_def)\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n-        os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-\n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n-\n-    np.random.seed(seed=args.seed)\n-    train_set = facenet.get_dataset(args.data_dir)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n-    if args.pretrained_model:\n-        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n-    \n-    if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n-        # Read the file containing the pairs used for testing\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-        # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-        \n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-\n-        # Placeholder for the learning rate\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        \n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        \n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        \n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n-        \n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n-                                    dtypes=[tf.string, tf.int64],\n-                                    shapes=[(3,), (3,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n-        \n-        nrof_preprocess_threads = 4\n-        images_and_labels = []\n-        for _ in range(nrof_preprocess_threads):\n-            filenames, label = input_queue.dequeue()\n-            images = []\n-            for filename in tf.unstack(filenames):\n-                file_contents = tf.read_file(filename)\n-                image = tf.image.decode_image(file_contents, channels=3)\n-                \n-                if args.random_crop:\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n-                else:\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n-                if args.random_flip:\n-                    image = tf.image.random_flip_left_right(image)\n-    \n-                #pylint: disable=no-member\n-                image.set_shape((args.image_size, args.image_size, 3))\n-                images.append(tf.image.per_image_standardization(image))\n-            images_and_labels.append([images, label])\n-    \n-        image_batch, labels_batch = tf.train.batch_join(\n-            images_and_labels, batch_size=batch_size_placeholder, \n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        labels_batch = tf.identity(labels_batch, \'label_batch\')\n-\n-        # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n-            weight_decay=args.weight_decay)\n-        \n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n-        \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n-\n-        # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n-\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables())\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-\n-        # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n-\n-        # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n-\n-        # Initialize variables\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-\n-            if args.pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n-\n-            # Training and validation loop\n-            epoch = 0\n-            while epoch < args.max_nrof_epochs:\n-                step = sess.run(global_step, feed_dict=None)\n-                epoch = step // args.epoch_size\n-                # Train for one epoch\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\n-\n-                # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n-\n-                # Evaluate on LFW\n-                if args.lfw_dir:\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n-\n-    return model_dir\n-\n-\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n-          embedding_size, anchor, positive, negative, triplet_loss):\n-    batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n-        lr = args.learning_rate\n-    else:\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-    while batch_number < args.epoch_size:\n-        # Sample people randomly from the dataset\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n-        \n-        print(\'Running forward pass on sampled images: \', end=\'\')\n-        start_time = time.time()\n-        nrof_examples = args.people_per_batch * args.images_per_person\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n-        for i in range(nrof_batches):\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\n-            emb_array[lab,:] = emb\n-        print(\'%.3f\' % (time.time()-start_time))\n-\n-        # Select triplets based on the embeddings\n-        print(\'Selecting suitable triplets for training\')\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n-            image_paths, args.people_per_batch, args.alpha)\n-        selection_time = time.time() - start_time\n-        print(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n-            (nrof_random_negs, nrof_triplets, selection_time))\n-\n-        # Perform training on the selected triplets\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n-        triplet_paths = list(itertools.chain(*triplets))\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n-        nrof_examples = len(triplet_paths)\n-        train_time = 0\n-        i = 0\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        loss_array = np.zeros((nrof_triplets,))\n-        summary = tf.Summary()\n-        step = 0\n-        while i < nrof_batches:\n-            start_time = time.time()\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n-            emb_array[lab,:] = emb\n-            loss_array[i] = err\n-            duration = time.time() - start_time\n-            print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\n-            batch_number += 1\n-            i += 1\n-            train_time += duration\n-            summary.value.add(tag=\'loss\', simple_value=err)\n-            \n-        # Add validation loss and accuracy to summary\n-        #pylint: disable=maybe-no-member\n-        summary.value.add(tag=\'time/selection\', simple_value=selection_time)\n-        summary_writer.add_summary(summary, step)\n-    return step\n-  \n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n-    """ Select the triplets for training\n-    """\n-    trip_idx = 0\n-    emb_start_idx = 0\n-    num_trips = 0\n-    triplets = []\n-    \n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n-    #  choosing the maximally violating example, as often done in structured output learning.\n-\n-    for i in xrange(people_per_batch):\n-        nrof_images = int(nrof_images_per_class[i])\n-        for j in xrange(1,nrof_images):\n-            a_idx = emb_start_idx + j - 1\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\n-                p_idx = emb_start_idx + pair\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n-                nrof_random_negs = all_neg.shape[0]\n-                if nrof_random_negs>0:\n-                    rnd_idx = np.random.randint(nrof_random_negs)\n-                    n_idx = all_neg[rnd_idx]\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n-                    #print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n-                    trip_idx += 1\n-\n-                num_trips += 1\n-\n-        emb_start_idx += nrof_images\n-\n-    np.random.shuffle(triplets)\n-    return triplets, num_trips, len(triplets)\n-\n-def sample_people(dataset, people_per_batch, images_per_person):\n-    nrof_images = people_per_batch * images_per_person\n-  \n-    # Sample classes from the dataset\n-    nrof_classes = len(dataset)\n-    class_indices = np.arange(nrof_classes)\n-    np.random.shuffle(class_indices)\n-    \n-    i = 0\n-    image_paths = []\n-    num_per_class = []\n-    sampled_class_indices = []\n-    # Sample images from these classes until we have enough\n-    while len(image_paths)<nrof_images:\n-        class_index = class_indices[i]\n-        nrof_images_in_class = len(dataset[class_index])\n-        image_indices = np.arange(nrof_images_in_class)\n-        np.random.shuffle(image_indices)\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n-        idx = image_indices[0:nrof_images_from_class]\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n-        sampled_class_indices += [class_index]*nrof_images_from_class\n-        image_paths += image_paths_for_class\n-        num_per_class.append(nrof_images_from_class)\n-        i+=1\n-  \n-    return image_paths, num_per_class\n-\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\n-    start_time = time.time()\n-    # Run forward pass to calculate embeddings\n-    print(\'Running forward pass on LFW images: \', end=\'\')\n-    \n-    nrof_images = len(actual_issame)*2\n-    assert(len(image_paths)==nrof_images)\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\n-    label_check_array = np.zeros((nrof_images,))\n-    for i in xrange(nrof_batches):\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n-        emb_array[lab,:] = emb\n-        label_check_array[lab] = 1\n-    print(\'%.3f\' % (time.time()-start_time))\n-    \n-    assert(np.all(label_check_array==1))\n-    \n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n-    \n-    print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    lfw_time = time.time() - start_time\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n-    summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n-\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n-    # Save the model checkpoint\n-    print(\'Saving variables\')\n-    start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-    save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n-    if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n-        start_time = time.time()\n-        saver.export_meta_graph(metagraph_filename)\n-        save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n-    summary_writer.add_summary(summary, step)\n-  \n-  \n-def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n-    \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--people_per_batch\', type=int,\n-        help=\'Number of people per batch.\', default=45)\n-    parser.add_argument(\'--images_per_person\', type=int,\n-        help=\'Number of images per person.\', default=40)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--alpha\', type=float,\n-        help=\'Positive to negative triplet distance margin.\', default=0.2)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-\n-    # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    return parser.parse_args(argv)\n-  \n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\ndeleted file mode 100644\nindex ac456c5..0000000\n--- a/src/validate_on_lfw.py\n+++ /dev/null\n@@ -1,164 +0,0 @@\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\n-in the same directory, and the metagraph should have the extension \'.meta\'.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import lfw\n-import os\n-import sys\n-from tensorflow.python.ops import data_flow_ops\n-from sklearn import metrics\n-from scipy.optimize import brentq\n-from scipy import interpolate\n-\n-def main(args):\n-  \n-    with tf.Graph().as_default():\n-      \n-        with tf.Session() as sess:\n-            \n-            # Read the file containing the pairs used for testing\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-\n-            # Get the paths for the corresponding images\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-            \n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n- \n-            nrof_preprocess_threads = 4\n-            image_size = (args.image_size, args.image_size)\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\n-                                        shapes=[(1,), (1,), (1,)],\n-                                        shared_name=None, name=None)\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'eval_enqueue_op\')\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-     \n-            # Load the model\n-            input_map = {\'image_batch\': image_batch, \'label_batch\': label_batch, \'phase_train\': phase_train_placeholder}\n-            facenet.load_model(args.model, input_map=input_map)\n-\n-            # Get output tensor\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-#              \n-            coord = tf.train.Coordinator()\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\n-                args.use_flipped_images, args.use_fixed_image_standardization)\n-\n-              \n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n-    # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n-    # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n-    nrof_flips = 2 if use_flipped_images else 1\n-    nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n-    control_array = np.zeros_like(labels_array, np.int32)\n-    if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n-    if use_flipped_images:\n-        # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n-    embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n-    nrof_batches = nrof_images // batch_size\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    lab_array = np.zeros((nrof_images,))\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n-        lab_array[lab] = lab\n-        emb_array[lab, :] = emb\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n-    if use_flipped_images:\n-        # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n-    else:\n-        embeddings = emb_array\n-\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    \n-    auc = metrics.auc(fpr, tpr)\n-    print(\'Area Under Curve (AUC): %1.3f\' % auc)\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n-    print(\'Equal Error Rate (EER): %1.3f\' % eer)\n-    \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--distance_metric\', type=int,\n-        help=\'Distance metric  0:euclidian, 1:cosine similarity.\', default=0)\n-    parser.add_argument(\'--use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))'